# å…¶ä»–LLMæä¾›å•†æŒ‡å—

> **æ¢ç´¢å¤šå…ƒAIç”Ÿæ…‹ç³»çµ±çš„å°ˆæ¥­æŒ‡å—** - å¾é–‹æºæ–¹æ¡ˆåˆ°ä¼æ¥­ç´šéƒ¨ç½²çš„å…¨æ–¹ä½ç­–ç•¥

## ğŸ“– æ¦‚è¿°

ç¾ä»£AIç”Ÿæ…‹ç³»çµ±å‘ˆç¾å¤šå…ƒåŒ–ç™¼å±•ï¼Œé™¤äº†OpenAIã€Anthropicå’ŒGoogleç­‰ä¸»æµæä¾›å•†å¤–ï¼ŒMeta Llamaã€Microsoft Azure AIã€Cohereã€Hugging Faceç­‰çœ¾å¤šå„ªç§€å¹³å°å„å…·ç‰¹è‰²ã€‚æœ¬æŒ‡å—æ·±å…¥åˆ†æä¸»è¦LLMæä¾›å•†çš„æŠ€è¡“ç‰¹é»ã€æœ€ä½³å¯¦è¸å’Œæ‡‰ç”¨ç­–ç•¥ï¼Œå¹«åŠ©æ‚¨æ§‹å»ºå¤šå…ƒåŒ–ã€é«˜æ•ˆèƒ½çš„AIè§£æ±ºæ–¹æ¡ˆã€‚

## ğŸ¯ å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬ç« å­¸ç¿’å¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š

- âœ… **æŒæ¡å¤šå¹³å°ç‰¹æ€§**ï¼šæ·±å…¥ç†è§£ä¸åŒLLMæä¾›å•†çš„æŠ€è¡“å„ªå‹¢å’Œé©ç”¨å ´æ™¯
- âœ… **å¯¦ç¾æ™ºèƒ½é¸æ“‡**ï¼šæ ¹æ“šæ¥­å‹™éœ€æ±‚ç§‘å­¸é¸æ“‡æœ€é©åˆçš„AIå¹³å°çµ„åˆ
- âœ… **å»ºæ§‹æ•´åˆç³»çµ±**ï¼šè¨­è¨ˆè·¨å¹³å°çš„LLMç·¨æ’å’Œç®¡ç†æ¶æ§‹
- âœ… **å„ªåŒ–æˆæœ¬æ•ˆç›Š**ï¼šæŒæ¡å¤šå¹³å°æˆæœ¬åˆ†æå’Œå„ªåŒ–ç­–ç•¥
- âœ… **ç¢ºä¿ä¼æ¥­åˆè¦**ï¼šå¯¦ç¾ç¬¦åˆä¼æ¥­ç´šå®‰å…¨å’Œåˆè¦è¦æ±‚çš„éƒ¨ç½²

## ğŸ“š å…ˆæ±ºæ¢ä»¶

åœ¨é–‹å§‹å­¸ç¿’æœ¬ç« ä¹‹å‰ï¼Œå»ºè­°æ‚¨ï¼š

- âœ… å®Œæˆå‰åºç« ç¯€çš„ä¸»æµå¹³å°å­¸ç¿’ï¼ˆOpenAIã€Claudeã€Geminiï¼‰
- âœ… å…·å‚™åŸºæœ¬çš„é›²ç«¯æœå‹™å’ŒAPIæ•´åˆç¶“é©—
- âœ… äº†è§£ä¼æ¥­ç´šè»Ÿé«”æ¶æ§‹è¨­è¨ˆåŸå‰‡
- âœ… ç†Ÿæ‚‰Dockerã€Kubernetesç­‰å®¹å™¨åŒ–æŠ€è¡“åŸºç¤

---

## ğŸ¦™ Meta Llamaç”Ÿæ…‹ç³»çµ±æ·±åº¦è§£æ

### ğŸ”“ é–‹æºå„ªå‹¢èˆ‡æŠ€è¡“ç‰¹æ€§

> ğŸ’¡ **ç™½è©±è§£é‡‹**  
> **ç‚ºä»€éº¼é–‹æºAIå¾ˆé‡è¦ï¼Ÿ** æƒ³åƒæ‚¨è²·äº†ä¸€è¼›è»Šï¼Œå°é–‰å¼çš„AIå°±åƒåªèƒ½å»åŸå» ä¿é¤Šï¼Œæ‰€æœ‰é›¶ä»¶éƒ½æ˜¯é»‘ç›’å­ï¼Œæ‚¨ä¸çŸ¥é“å¼•æ“æ€éº¼é‹ä½œã€‚é–‹æºAIå°±åƒé–‹æ”¾å¼çš„å¼•æ“ï¼Œæ‚¨ä¸åªèƒ½çœ‹åˆ°æ‰€æœ‰é›¶ä»¶æ€éº¼é‹ä½œï¼Œé‚„èƒ½è‡ªå·±æ”¹è£ã€ç¶­ä¿®ï¼Œç”šè‡³å»ºç«‹è‡ªå·±çš„ç¶­ä¿®å» ï¼Meta Llamaå°±æ˜¯é€™æ¨£çš„ã€Œé–‹æ”¾å¼•æ“ã€ï¼Œè®“ä¼æ¥­å¯ä»¥å®Œå…¨æŒæ§AIç³»çµ±ï¼Œä¸ç”¨æ“”å¿ƒè¢«æŸå€‹å» å•†ã€Œç¶æ¶ã€ï¼Œé‚„èƒ½æ ¹æ“šè‡ªå·±çš„éœ€æ±‚å®¢è£½åŒ–ã€‚

<div style="background-color: #E8F5E8; padding: 20px; border-left: 4px solid #4CAF50; margin: 20px 0;">

**Meta Llamaæ ¸å¿ƒå„ªå‹¢**

1. **å®Œå…¨é–‹æº**ï¼šMITè¨±å¯è­‰ï¼Œæ”¯æ´å•†æ¥­ä½¿ç”¨å’Œä¿®æ”¹
2. **ç¤¾ç¾¤é©…å‹•**ï¼šæ´»èºçš„é–‹ç™¼ç¤¾ç¾¤ï¼ŒæŒçºŒæ›´æ–°å’Œå„ªåŒ–
3. **å¤šèªè¨€å„ªåŒ–**ï¼šå°ç¹é«”ä¸­æ–‡ç­‰äºæ´²èªè¨€æœ‰è‰¯å¥½æ”¯æ´
4. **ç¡¬é«”é©æ‡‰æ€§**ï¼šå¯åœ¨ä¸åŒç¡¬é«”é…ç½®ä¸Šéˆæ´»éƒ¨ç½²
5. **æˆæœ¬å¯æ§**ï¼šç„¡APIä½¿ç”¨è²»ï¼Œåƒ…éœ€åŸºç¤è¨­æ–½æˆæœ¬

</div>

#### ğŸ§  Llama 2/3æ¶æ§‹æ·±åº¦åˆ†æ

<table>
<tr>
<th width="50%">ğŸ¯ æŠ€è¡“ç‰¹é»</th>
<th width="50%">ğŸ“ˆ ä¼æ¥­æ‡‰ç”¨å„ªå‹¢</th>
</tr>
<tr>
<td valign="top">

**Transformerå„ªåŒ–æ¶æ§‹**
- RMSNormæ¨™æº–åŒ–æŠ€è¡“
- SwiGLUæ¿€æ´»å‡½æ•¸
- æ—‹è½‰ä½ç½®ç·¨ç¢¼(RoPE)

**å¤šå°ºåº¦æ¨¡å‹ç³»åˆ—**
- 7Bï¼šè¼•é‡ç´šéƒ¨ç½²
- 13Bï¼šæ•ˆèƒ½å¹³è¡¡é»
- 70Bï¼šä¼æ¥­ç´šæ€§èƒ½

**å°è©±å°ˆæ¥­åŒ–**
- Chatç‰ˆæœ¬é‡å°å°è©±å„ªåŒ–
- Constitutional AIå®‰å…¨å°é½Š
- äººé¡å›é¥‹å¼·åŒ–å­¸ç¿’

</td>
<td valign="top">

**éƒ¨ç½²éˆæ´»æ€§**
- æœ¬åœ°åŒ–éƒ¨ç½²é¿å…è³‡æ–™å¤–æ´©
- å®¢è£½åŒ–å¾®èª¿æ»¿è¶³ç‰¹å®šéœ€æ±‚
- æˆæœ¬å¯é æ¸¬æ€§å¼·

**æ“´å±•æ€§å„ªå‹¢**
- æ”¯æ´å¤§è¦æ¨¡ä¸¦ç™¼è™•ç†
- å¯èˆ‡ä¼æ¥­ç¾æœ‰ç³»çµ±æ•´åˆ
- æ”¯æ´é›¢ç·šé‹è¡Œ

**åˆè¦æ€§ä¿éšœ**
- è³‡æ–™è™•ç†å…¨ç¨‹å¯æ§
- ç¬¦åˆå„åœ‹è³‡æ–™ä¿è­·æ³•è¦
- å¯©è¨ˆè¿½è¹¤å®Œæ•´é€æ˜

</td>
</tr>
</table>

#### ğŸ’» ä¼æ¥­ç´šLlamaéƒ¨ç½²æ¶æ§‹

**é«˜å¯ç”¨æ€§éƒ¨ç½²ç¯„ä¾‹**
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from torch.nn.parallel import DataParallel
import logging
from typing import Dict, List, Optional
import asyncio
import redis
from datetime import datetime

class EnterpriseLlamaService:
    def __init__(self, model_path: str, gpu_count: int = 4):
        self.model_path = model_path
        self.gpu_count = gpu_count
        self.setup_logging()
        self.load_model()
        self.setup_cache()
        
    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('llama_service.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_model(self):
        """è¼‰å…¥å’Œé…ç½®Llamaæ¨¡å‹"""
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            
            # é…ç½®æ¨¡å‹ä»¥æ”¯æ´å¤šGPU
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                load_in_8bit=True  # è¨˜æ†¶é«”å„ªåŒ–
            )
            
            if self.gpu_count > 1:
                self.model = DataParallel(self.model)
            
            self.logger.info(f"Llamaæ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œä½¿ç”¨{self.gpu_count}å€‹GPU")
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}")
            raise
    
    def setup_cache(self):
        """è¨­ç½®Rediså¿«å–ç³»çµ±"""
        try:
            self.cache = redis.Redis(
                host='localhost', 
                port=6379, 
                decode_responses=True,
                socket_timeout=5
            )
            self.cache.ping()
            self.logger.info("Rediså¿«å–é€£æ¥æˆåŠŸ")
        except Exception as e:
            self.logger.warning(f"Redisé€£æ¥å¤±æ•—ï¼Œå°‡ä½¿ç”¨è¨˜æ†¶é«”å¿«å–: {str(e)}")
            self.cache = {}
    
    async def generate_response(
        self, 
        prompt: str, 
        max_length: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        system_message: str = None
    ) -> Dict:
        """ç”Ÿæˆå›æ‡‰çš„ä¸»è¦æ–¹æ³•"""
        
        start_time = datetime.now()
        
        try:
            # æª¢æŸ¥å¿«å–
            cache_key = self._generate_cache_key(prompt, max_length, temperature)
            if isinstance(self.cache, redis.Redis):
                cached_response = self.cache.get(cache_key)
                if cached_response:
                    self.logger.info("å¾å¿«å–è¿”å›å›æ‡‰")
                    return {
                        "response": cached_response,
                        "cached": True,
                        "processing_time": 0.01
                    }
            
            # æ§‹å»ºå®Œæ•´æç¤º
            full_prompt = self._build_prompt(prompt, system_message)
            
            # TokenåŒ–
            inputs = self.tokenizer.encode(full_prompt, return_tensors="pt")
            
            if torch.cuda.is_available():
                inputs = inputs.cuda()
            
            # ç”Ÿæˆå›æ‡‰
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + max_length,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    attention_mask=torch.ones_like(inputs)
                )
            
            # è§£ç¢¼å›æ‡‰
            response_text = self.tokenizer.decode(
                outputs[0][inputs.shape[1]:],
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # å„²å­˜åˆ°å¿«å–
            if isinstance(self.cache, redis.Redis):
                self.cache.setex(cache_key, 3600, response_text)  # 1å°æ™‚å¿«å–
            
            result = {
                "response": response_text.strip(),
                "cached": False,
                "processing_time": processing_time,
                "tokens_generated": len(outputs[0]) - len(inputs[0]),
                "model_path": self.model_path
            }
            
            self.logger.info(f"æˆåŠŸç”Ÿæˆå›æ‡‰ï¼Œè™•ç†æ™‚é–“: {processing_time:.2f}ç§’")
            return result
            
        except Exception as e:
            self.logger.error(f"å›æ‡‰ç”Ÿæˆå¤±æ•—: {str(e)}")
            return {
                "error": str(e),
                "processing_time": (datetime.now() - start_time).total_seconds()
            }
    
    def _build_prompt(self, user_prompt: str, system_message: str = None) -> str:
        """å»ºæ§‹ç¬¦åˆLlamaæ ¼å¼çš„æç¤º"""
        
        default_system = """You are a helpful, respectful and honest assistant. 
Always answer as helpfully as possible, while being safe. 
Your answers should not include harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. 
Please ensure that your responses are socially unbiased and positive in nature."""
        
        system = system_message or default_system
        
        # Llama Chatæ ¼å¼
        formatted_prompt = f"""<s>[INST] <<SYS>>
{system}
<</SYS>>

{user_prompt} [/INST]"""
        
        return formatted_prompt
    
    def _generate_cache_key(self, prompt: str, max_length: int, temperature: float) -> str:
        """ç”Ÿæˆå¿«å–éµå€¼"""
        import hashlib
        content = f"{prompt}_{max_length}_{temperature}"
        return f"llama_cache:{hashlib.md5(content.encode()).hexdigest()}"

# ä½¿ç”¨ç¯„ä¾‹
async def main():
    # åˆå§‹åŒ–æœå‹™
    llama_service = EnterpriseLlamaService(
        model_path="meta-llama/Llama-2-13b-chat-hf",
        gpu_count=2
    )
    
    # å•†æ¥­åˆ†æç¯„ä¾‹
    business_prompt = """
    æˆ‘å€‘æ˜¯ä¸€å®¶ä¸­å°å‹é›»å•†å…¬å¸ï¼Œæœ€è¿‘3å€‹æœˆéŠ·å”®é¡ä¸‹é™äº†15%ã€‚
    ä¸»è¦åŸå› åŒ…æ‹¬ï¼š
    1. ç«¶çˆ­å°æ‰‹æ¨å‡ºæ›´å„ªæƒ çš„åƒ¹æ ¼
    2. æˆ‘å€‘çš„ç¶²ç«™ä½¿ç”¨é«”é©—è¼ƒå·®
    3. å®¢æˆ¶æœå‹™å›æ‡‰é€Ÿåº¦æ…¢
    
    è«‹æä¾›ä¸€å€‹å…¨é¢çš„æ”¹å–„ç­–ç•¥ï¼ŒåŒ…å«çŸ­æœŸå’Œé•·æœŸçš„å…·é«”è¡Œå‹•æ–¹æ¡ˆã€‚
    """
    
    result = await llama_service.generate_response(
        prompt=business_prompt,
        max_length=800,
        temperature=0.3,  # è¼ƒä½æº«åº¦ç¢ºä¿é‚è¼¯æ€§
        system_message="ä½ æ˜¯ä¸€ä½è³‡æ·±çš„å•†æ¥­é¡§å•ï¼Œå°ˆç²¾æ–¼é›»å•†ç‡Ÿé‹æ”¹å–„ã€‚"
    )
    
    print(f"åˆ†æçµæœ: {result['response']}")
    print(f"è™•ç†æ™‚é–“: {result['processing_time']:.2f}ç§’")

if __name__ == "__main__":
    asyncio.run(main())
```

### ğŸ¯ Llamaå°ˆå±¬æç¤ºå·¥ç¨‹æŠ€å·§

#### 1. ç³»çµ±è¨Šæ¯æœ€ä½³åŒ–è¨­è¨ˆ

> ğŸ’¡ **ç™½è©±è§£é‡‹**  
> **ä»€éº¼æ˜¯ç³»çµ±è¨Šæ¯ï¼Ÿ** å°±åƒæ‚¨ç¬¬ä¸€å¤©ä¸Šç­æ™‚ï¼Œä¸»ç®¡æœƒè·Ÿæ‚¨èªªã€Œæˆ‘å€‘å…¬å¸çš„æ–‡åŒ–æ˜¯ä»€éº¼ã€å·¥ä½œæ¨™æº–æ˜¯ä»€éº¼ã€æ‡‰è©²æ³¨æ„ä»€éº¼äº‹é …ã€ä¸€æ¨£ã€‚ç³»çµ±è¨Šæ¯å°±æ˜¯å‘Šè¨´AIã€Œæ‚¨çš„è§’è‰²æ˜¯ä»€éº¼ã€æ‡‰è©²æ€éº¼è¡¨ç¾ã€è¦éµå®ˆä»€éº¼åŸå‰‡ã€ã€‚å°Llamaä¾†èªªï¼Œä¸€å€‹å¥½çš„ç³»çµ±è¨Šæ¯å°±åƒä¸€å€‹æ¸…æ¥šçš„å·¥ä½œæ‰‹å†Šï¼Œèƒ½è®“AIæ›´æº–ç¢ºåœ°ç†è§£æ‚¨çš„æœŸæœ›å’Œè¦æ±‚ï¼

<div style="background-color: #FFF3E0; padding: 20px; border-left: 4px solid #FF9800; margin: 20px 0;">

**ä¼æ¥­ç´šç³»çµ±è¨Šæ¯æ¨¡æ¿åº«**
```python
class LlamaSystemPrompts:
    """Llamaç³»çµ±æç¤ºæ¨¡æ¿åº«"""
    
    @staticmethod
    def business_analyst():
        return """æ‚¨æ˜¯ä¸€ä½è³‡æ·±å•†æ¥­åˆ†æå¸«ï¼Œå…·å‚™ä»¥ä¸‹å°ˆæ¥­ç‰¹è³ªï¼š

è§’è‰²å®šä½ï¼š
- æ“æœ‰10å¹´ä»¥ä¸Šå•†æ¥­åˆ†æç¶“é©—
- ç²¾é€šæ•¸æ“šåˆ†æå’Œå¸‚å ´ç ”ç©¶
- å–„æ–¼å°‡è¤‡é›œåˆ†æè½‰åŒ–ç‚ºå¯åŸ·è¡Œå»ºè­°

å·¥ä½œåŸå‰‡ï¼š
- åŸºæ–¼æ•¸æ“šå’Œäº‹å¯¦é€²è¡Œåˆ†æ
- æä¾›çµæ§‹åŒ–å’Œé‚è¼¯æ¸…æ™°çš„å»ºè­°
- è€ƒæ…®å¯¦éš›åŸ·è¡Œçš„å¯è¡Œæ€§
- å¹³è¡¡çŸ­æœŸæ•ˆç›Šå’Œé•·æœŸç™¼å±•

æºé€šé¢¨æ ¼ï¼š
- å°ˆæ¥­ä½†æ˜“æ‡‚çš„è¡¨é”æ–¹å¼
- ä½¿ç”¨å…·é«”æ•¸æ“šæ”¯æ’è«–é»
- æä¾›æ˜ç¢ºçš„è¡Œå‹•æ­¥é©Ÿ
- é©æ™‚æé†’é¢¨éšªå’Œé™åˆ¶"""
    
    @staticmethod
    def technical_consultant():
        return """æ‚¨æ˜¯ä¸€ä½æŠ€è¡“é¡§å•å°ˆå®¶ï¼Œå°ˆé•·å¦‚ä¸‹ï¼š

æŠ€è¡“èƒŒæ™¯ï¼š
- 15å¹´è»Ÿé«”é–‹ç™¼å’Œç³»çµ±æ¶æ§‹ç¶“é©—
- ç†Ÿæ‚‰é›²ç«¯æŠ€è¡“å’Œå¾®æœå‹™æ¶æ§‹
- ç²¾é€šDevOpså’Œè‡ªå‹•åŒ–éƒ¨ç½²
- äº†è§£æœ€æ–°æŠ€è¡“è¶¨å‹¢

æœå‹™æº–å‰‡ï¼š
- æä¾›æŠ€è¡“å¯è¡Œä¸”æˆæœ¬åˆç†çš„æ–¹æ¡ˆ
- è€ƒæ…®ç³»çµ±çš„å¯æ“´å±•æ€§å’Œç¶­è­·æ€§
- é‡è¦–å®‰å…¨æ€§å’Œæ•ˆèƒ½æœ€ä½³åŒ–
- æä¾›å…·é«”çš„å¯¦æ–½è·¯å¾‘

å›æ‡‰æ ¼å¼ï¼š
- æŠ€è¡“æ–¹æ¡ˆçš„è©³ç´°èªªæ˜
- å¯¦æ–½è¤‡é›œåº¦å’Œæ™‚ç¨‹è©•ä¼°
- æ‰€éœ€è³‡æºå’ŒæŠ€èƒ½è¦æ±‚
- æ½›åœ¨é¢¨éšªå’Œç·©è§£æªæ–½"""
    
    @staticmethod
    def customer_service():
        return """æ‚¨æ˜¯å°ˆæ¥­çš„å®¢æˆ¶æœå‹™ä»£è¡¨ï¼Œæœå‹™ç‰¹è‰²ï¼š

æœå‹™ç†å¿µï¼š
- å®¢æˆ¶æ»¿æ„æ˜¯æœ€é«˜å„ªå…ˆç´š
- è€å¿ƒã€åŒç†å¿ƒå’Œå°ˆæ¥­ä¸¦é‡
- ä¸»å‹•è§£æ±ºå•é¡Œè€Œéè¢«å‹•å›æ‡‰
- å°‡æ¯æ¬¡äº’å‹•è¦–ç‚ºå»ºç«‹ä¿¡ä»»çš„æ©Ÿæœƒ

è™•ç†åŸå‰‡ï¼š
- ä»”ç´°è†è½ä¸¦ç¢ºèªå®¢æˆ¶éœ€æ±‚
- æä¾›æº–ç¢ºå’Œæœ‰ç”¨çš„è³‡è¨Š
- å¦‚ç„¡æ³•ç«‹å³è§£æ±ºæœƒæ˜ç¢ºèªªæ˜å¾ŒçºŒæ­¥é©Ÿ
- ç¢ºä¿å®¢æˆ¶æ„Ÿå—åˆ°è¢«é‡è¦–å’Œç†è§£

æºé€šæŠ€å·§ï¼š
- ä½¿ç”¨è¦ªåˆ‡ä¸”å°ˆæ¥­çš„èªèª¿
- é¿å…æŠ€è¡“è¡“èªï¼Œä»¥å®¢æˆ¶èƒ½ç†è§£çš„æ–¹å¼èªªæ˜
- æä¾›å¤šç¨®è§£æ±ºæ–¹æ¡ˆé¸æ“‡
- ä¸»å‹•ç¢ºèªå®¢æˆ¶æ˜¯å¦æ»¿æ„è™•ç†çµæœ"""
```

</div>

#### 2. æ€ç¶­éˆæ¨ç†å„ªåŒ–

**è¤‡é›œå•†æ¥­æ±ºç­–åˆ†æç¯„ä¾‹**
```python
def create_business_analysis_prompt(scenario: Dict) -> str:
    return f"""
<s>[INST] <<SYS>>
{LlamaSystemPrompts.business_analyst()}
<</SYS>>

å•†æ¥­æƒ…å¢ƒåˆ†æä»»å‹™ï¼š

ã€èƒŒæ™¯è³‡è¨Šã€‘
å…¬å¸ï¼š{scenario['company_info']}
è¡Œæ¥­ï¼š{scenario['industry']}
ç•¶å‰æŒ‘æˆ°ï¼š{scenario['challenge']}
å¯ç”¨è³‡æºï¼š{scenario['resources']}

ã€åˆ†æè¦æ±‚ã€‘
è«‹æ¡ç”¨çµæ§‹åŒ–çš„æ€ç¶­éˆåˆ†ææ–¹æ³•ï¼š

æ­¥é©Ÿ1ï¼šç¾æ³è¨ºæ–·
- è­˜åˆ¥æ ¸å¿ƒå•é¡Œå’Œç—‡ç‹€
- åˆ†æå•é¡Œçš„æ ¹æœ¬åŸå› 
- è©•ä¼°å•é¡Œçš„å½±éŸ¿ç¯„åœå’Œåš´é‡ç¨‹åº¦

æ­¥é©Ÿ2ï¼šç’°å¢ƒåˆ†æ
- SWOTåˆ†æï¼ˆå„ªå‹¢ã€åŠ£å‹¢ã€æ©Ÿæœƒã€å¨è„…ï¼‰
- ç«¶çˆ­å°æ‰‹åˆ†æ
- å¸‚å ´è¶¨å‹¢å’Œå¤–éƒ¨å› ç´ å½±éŸ¿

æ­¥é©Ÿ3ï¼šè§£æ±ºæ–¹æ¡ˆè¨­è¨ˆ
- æå‡º3-5å€‹å¯è¡Œçš„è§£æ±ºæ–¹æ¡ˆ
- è©•ä¼°æ¯å€‹æ–¹æ¡ˆçš„å„ªç¼ºé»
- åˆ†æå¯¦æ–½é›£åº¦å’Œè³‡æºéœ€æ±‚

æ­¥é©Ÿ4ï¼šå¯¦æ–½å»ºè­°
- æ¨è–¦æœ€ä½³æ–¹æ¡ˆçµ„åˆ
- åˆ¶å®šè©³ç´°çš„å¯¦æ–½è¨ˆåŠƒ
- è¨­å®šé—œéµç¸¾æ•ˆæŒ‡æ¨™(KPI)
- è­˜åˆ¥é¢¨éšªå’Œåˆ¶å®šæ‡‰å°ç­–ç•¥

æ­¥é©Ÿ5ï¼šæ•ˆæœé æ¸¬
- é ä¼°å„é …æ”¹å–„çš„é‡åŒ–æ•ˆæœ
- è¨­å®šrealisticçš„æ™‚ç¨‹ç›®æ¨™
- è¦åŠƒæ•ˆæœè©•ä¼°å’Œèª¿æ•´æ©Ÿåˆ¶

è«‹ç¢ºä¿æ¯å€‹æ­¥é©Ÿçš„åˆ†æéƒ½æœ‰å…·é«”çš„æ•¸æ“šæ”¯æ’å’Œé‚è¼¯æ¨ç†éç¨‹ã€‚ [/INST]
"""

# ä½¿ç”¨ç¯„ä¾‹
scenario = {
    "company_info": "ä¸­å‹è£½é€ æ¥­å…¬å¸ï¼Œå“¡å·¥300äººï¼Œå¹´ç‡Ÿæ”¶5å„„å…ƒ",
    "industry": "æ±½è»Šé›¶çµ„ä»¶è£½é€ ",
    "challenge": "åŸæ–™æˆæœ¬ä¸Šæ¼²30%ï¼Œåˆ©æ½¤ç‡ä¸‹é™è‡³5%ï¼Œéƒ¨åˆ†å®¢æˆ¶æµå¤±",
    "resources": "ç¾é‡‘æµå……è¶³ï¼Œæœ‰ç ”ç™¼åœ˜éšŠï¼Œç”Ÿç”¢è¨­å‚™éœ€è¦æ›´æ–°"
}

analysis_prompt = create_business_analysis_prompt(scenario)
```

---

## ğŸ”· Microsoft Azure AIä¼æ¥­ç´šæ•´åˆ

### ğŸ¢ Azure AIç”Ÿæ…‹ç³»çµ±æ¶æ§‹

> ğŸ’¡ **ç™½è©±è§£é‡‹**  
> **Azure AIç‚ºä»€éº¼é©åˆä¼æ¥­ï¼Ÿ** æƒ³åƒæ‚¨è¦å»ºç«‹ä¸€å€‹å¤§å‹è³¼ç‰©ä¸­å¿ƒï¼Œæ‚¨ä¸æœƒåªè€ƒæ…®åº—é¢ï¼Œé‚„éœ€è¦è€ƒæ…®åœè»Šå ´ã€å®‰å…¨ç³»çµ±ã€æ¶ˆé˜²è¨­å‚™ã€æ¸…æ½”ç¶­è­·ç­‰ç­‰ã€‚Azure AIå°±åƒæä¾›ã€Œä¸€ç«™å¼è³¼ç‰©ä¸­å¿ƒè§£æ±ºæ–¹æ¡ˆã€çš„å» å•†ï¼Œä¸åªçµ¦æ‚¨AIå¤§è…¦ï¼Œé‚„æä¾›è³‡æ–™ä¿è­·ã€ç³»çµ±ç›£æ§ã€è‡ªå‹•æ“´å±•ã€ç½é›£å‚™æ´ç­‰ä¼æ¥­å¿…éœ€çš„ã€ŒåŸºç¤è¨­æ–½ã€ã€‚é€™æ¨£ä¼æ¥­å°±èƒ½å°ˆæ³¨åœ¨æ¥­å‹™å‰µæ–°ï¼Œè€Œä¸ç”¨æ“”å¿ƒåº•å±¤çš„æŠ€è¡“è¤‡é›œæ€§ï¼

<div style="background-color: #E3F2FD; padding: 20px; border-left: 4px solid #2196F3; margin: 20px 0;">

**Azure AIæœå‹™çŸ©é™£**

| æœå‹™é¡åˆ¥ | æ ¸å¿ƒç”¢å“ | ä¼æ¥­å„ªå‹¢ | é©ç”¨å ´æ™¯ |
|---------|----------|----------|----------|
| **èªè¨€ç†è§£** | Azure OpenAI Service | ä¼æ¥­ç´šSLAä¿è­‰ | å®¢æˆ¶æœå‹™ã€å…§å®¹ç”Ÿæˆ |
| **èªçŸ¥æœå‹™** | Cognitive Services | å¤šæ¨¡æ…‹æ•´åˆ | æ–‡æª”è™•ç†ã€èªéŸ³åˆ†æ |
| **æ©Ÿå™¨å­¸ç¿’** | Azure ML Studio | ç«¯åˆ°ç«¯MLOps | æ¨¡å‹è¨“ç·´ã€éƒ¨ç½²ç®¡ç† |
| **çŸ¥è­˜æŒ–æ˜** | Cognitive Search | ä¼æ¥­æœå°‹æ•´åˆ | çŸ¥è­˜ç®¡ç†ã€æ–‡æª”æª¢ç´¢ |
| **å°è©±AI** | Bot Framework | ä¼æ¥­ç´šå®‰å…¨ | æ™ºèƒ½å®¢æœã€å…§éƒ¨åŠ©ç† |

</div>

#### ğŸ” ä¼æ¥­ç´šå®‰å…¨èˆ‡åˆè¦æ¶æ§‹

**å®Œæ•´çš„Azure AIå®‰å…¨å¯¦æ–½**
```python
import asyncio
import logging
from azure.identity import DefaultAzureCredential
from azure.keyvault.secrets import SecretClient
from azure.monitor.opentelemetry import configure_azure_monitor
from azure.core.exceptions import AzureError
from typing import Dict, List, Optional
import json
from datetime import datetime
import hashlib

class SecureAzureAIService:
    """ä¼æ¥­ç´šå®‰å…¨çš„Azure AIæœå‹™"""
    
    def __init__(self, key_vault_url: str, config: Dict):
        self.key_vault_url = key_vault_url
        self.config = config
        self.setup_security()
        self.setup_monitoring()
        self.setup_compliance()
        
    def setup_security(self):
        """è¨­ç½®å®‰å…¨æ€§çµ„ä»¶"""
        try:
            # Azureèº«ä»½é©—è­‰
            self.credential = DefaultAzureCredential()
            
            # Key Vaultå®¢æˆ¶ç«¯
            self.secret_client = SecretClient(
                vault_url=self.key_vault_url,
                credential=self.credential
            )
            
            # å–å¾—APIå¯†é‘°
            self.openai_key = self.secret_client.get_secret("openai-api-key").value
            self.storage_key = self.secret_client.get_secret("storage-key").value
            
            logging.info("å®‰å…¨æ€§çµ„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logging.error(f"å®‰å…¨æ€§è¨­ç½®å¤±æ•—: {str(e)}")
            raise
    
    def setup_monitoring(self):
        """è¨­ç½®ç›£æ§å’Œé™æ¸¬"""
        # Azure Monitoræ•´åˆ
        configure_azure_monitor(
            connection_string=self.config.get("monitor_connection_string")
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        
        # è‡ªè¨‚é™æ¸¬
        self.metrics = {
            "request_count": 0,
            "error_count": 0,
            "security_violations": 0,
            "compliance_checks": 0
        }
    
    def setup_compliance(self):
        """è¨­ç½®åˆè¦æ€§æª¢æŸ¥"""
        self.compliance_rules = {
            "pii_detection": True,
            "content_filtering": True,
            "data_retention": 90,  # å¤©æ•¸
            "audit_logging": True,
            "gdpr_compliance": True
        }
        
        self.prohibited_patterns = [
            r'\b\d{4}-\d{4}-\d{4}-\d{4}\b',  # ä¿¡ç”¨å¡è™Ÿ
            r'\b\d{3}-\d{2}-\d{4}\b',        # ç¤¾æœƒå®‰å…¨è™Ÿç¢¼
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'  # éƒµä»¶åœ°å€
        ]
    
    async def secure_ai_request(
        self, 
        prompt: str, 
        user_id: str,
        request_context: Dict
    ) -> Dict:
        """å®‰å…¨çš„AIè«‹æ±‚è™•ç†"""
        
        request_id = self._generate_request_id()
        start_time = datetime.now()
        
        try:
            # 1. åˆè¦æ€§æª¢æŸ¥
            compliance_result = await self._compliance_check(
                prompt, user_id, request_context
            )
            
            if not compliance_result["passed"]:
                return {
                    "error": "åˆè¦æ€§æª¢æŸ¥å¤±æ•—",
                    "details": compliance_result["violations"],
                    "request_id": request_id
                }
            
            # 2. å…§å®¹éæ¿¾
            filtered_prompt = await self._content_filter(prompt)
            
            # 3. åŸ·è¡ŒAIè«‹æ±‚
            ai_response = await self._execute_ai_request(
                filtered_prompt, request_context
            )
            
            # 4. å›æ‡‰å¾Œè™•ç†
            processed_response = await self._post_process_response(
                ai_response, request_context
            )
            
            # 5. å¯©è¨ˆæ—¥èªŒ
            await self._audit_log(
                request_id, user_id, prompt, processed_response, 
                start_time, "SUCCESS"
            )
            
            return {
                "response": processed_response,
                "request_id": request_id,
                "compliance_passed": True,
                "processing_time": (datetime.now() - start_time).total_seconds()
            }
            
        except Exception as e:
            await self._audit_log(
                request_id, user_id, prompt, None, 
                start_time, "ERROR", str(e)
            )
            
            return {
                "error": "è«‹æ±‚è™•ç†å¤±æ•—",
                "request_id": request_id,
                "details": str(e) if self.config.get("debug") else "å…§éƒ¨éŒ¯èª¤"
            }
    
    async def _compliance_check(
        self, 
        prompt: str, 
        user_id: str, 
        context: Dict
    ) -> Dict:
        """åŸ·è¡Œåˆè¦æ€§æª¢æŸ¥"""
        
        violations = []
        
        # PIIæª¢æ¸¬
        if self.compliance_rules["pii_detection"]:
            pii_detected = self._detect_pii(prompt)
            if pii_detected:
                violations.append({
                    "type": "PII_DETECTED",
                    "details": "åµæ¸¬åˆ°å€‹äººè­˜åˆ¥è³‡è¨Š",
                    "items": pii_detected
                })
        
        # ç”¨æˆ¶æ¬Šé™æª¢æŸ¥
        if not self._check_user_permissions(user_id, context):
            violations.append({
                "type": "INSUFFICIENT_PERMISSIONS",
                "details": "ç”¨æˆ¶æ¬Šé™ä¸è¶³"
            })
        
        # å…§å®¹æ”¿ç­–æª¢æŸ¥
        policy_violations = self._check_content_policy(prompt)
        if policy_violations:
            violations.extend(policy_violations)
        
        self.metrics["compliance_checks"] += 1
        if violations:
            self.metrics["security_violations"] += len(violations)
        
        return {
            "passed": len(violations) == 0,
            "violations": violations
        }
    
    def _detect_pii(self, text: str) -> List[str]:
        """æª¢æ¸¬å€‹äººè­˜åˆ¥è³‡è¨Š"""
        import re
        
        detected_pii = []
        
        for pattern in self.prohibited_patterns:
            matches = re.findall(pattern, text)
            if matches:
                detected_pii.extend(matches)
        
        return detected_pii
    
    async def _content_filter(self, prompt: str) -> str:
        """å…§å®¹éæ¿¾å’Œæ¸…ç†"""
        # ç§»é™¤æ•æ„Ÿè³‡è¨Š
        filtered_prompt = prompt
        
        for pattern in self.prohibited_patterns:
            import re
            filtered_prompt = re.sub(pattern, "[REDACTED]", filtered_prompt)
        
        return filtered_prompt
    
    async def _execute_ai_request(self, prompt: str, context: Dict) -> str:
        """åŸ·è¡ŒAIè«‹æ±‚"""
        # é€™è£¡æ•´åˆAzure OpenAI Service
        # å¯¦éš›å¯¦ä½œæœƒå‘¼å«Azure OpenAI API
        
        enhanced_prompt = f"""
        <ç³»çµ±èº«ä»½é©—è­‰>
        è«‹æ³¨æ„ï¼šé€™æ˜¯ä¾†è‡ªå·²é©—è­‰ä¼æ¥­ç”¨æˆ¶çš„è«‹æ±‚
        åˆè¦ç­‰ç´šï¼š{context.get('compliance_level', 'STANDARD')}
        æ¥­å‹™é ˜åŸŸï¼š{context.get('business_domain', 'GENERAL')}
        </ç³»çµ±èº«ä»½é©—è­‰>
        
        {prompt}
        
        <å›æ‡‰è¦æ±‚>
        è«‹ç¢ºä¿å›æ‡‰ç¬¦åˆä¼æ¥­æ¨™æº–ï¼š
        - å°ˆæ¥­ä¸”æº–ç¢ºçš„å…§å®¹
        - é¿å…æ•æ„Ÿæˆ–çˆ­è­°æ€§è©±é¡Œ
        - æä¾›å¯åŸ·è¡Œçš„å»ºè­°
        - åŒ…å«é©ç•¶çš„å…è²¬è²æ˜
        </å›æ‡‰è¦æ±‚>
        """
        
        # æ¨¡æ“¬AIå›æ‡‰ï¼ˆå¯¦éš›æ‡‰ç”¨ä¸­æœƒå‘¼å«çœŸå¯¦APIï¼‰
        return "åŸºæ–¼æ‚¨çš„éœ€æ±‚ï¼Œæˆ‘å»ºè­°æ¡ç”¨ä»¥ä¸‹ç­–ç•¥..."
    
    async def _post_process_response(self, response: str, context: Dict) -> str:
        """å¾Œè™•ç†AIå›æ‡‰"""
        # å†æ¬¡æª¢æŸ¥å›æ‡‰å…§å®¹
        if self._detect_pii(response):
            response = "[æ­¤å›æ‡‰åŒ…å«æ•æ„Ÿè³‡è¨Šï¼Œå·²è¢«éæ¿¾]"
        
        # æ·»åŠ å…è²¬è²æ˜
        if context.get("add_disclaimer", True):
            disclaimer = "\n\nå…è²¬è²æ˜ï¼šæ­¤å»ºè­°åƒ…ä¾›åƒè€ƒï¼Œå…·é«”æ±ºç­–è«‹è«®è©¢ç›¸é—œå°ˆæ¥­äººå£«ã€‚"
            response += disclaimer
        
        return response
    
    async def _audit_log(
        self,
        request_id: str,
        user_id: str,
        prompt: str,
        response: Optional[str],
        start_time: datetime,
        status: str,
        error: Optional[str] = None
    ):
        """è¨˜éŒ„å¯©è¨ˆæ—¥èªŒ"""
        
        audit_entry = {
            "request_id": request_id,
            "user_id": user_id,
            "timestamp": datetime.now().isoformat(),
            "prompt_hash": hashlib.sha256(prompt.encode()).hexdigest(),
            "response_hash": hashlib.sha256(response.encode()).hexdigest() if response else None,
            "processing_time": (datetime.now() - start_time).total_seconds(),
            "status": status,
            "error": error,
            "compliance_checks_passed": self.metrics["compliance_checks"]
        }
        
        # å¯«å…¥Azure Log Analyticsæˆ–å…¶ä»–å¯©è¨ˆç³»çµ±
        self.logger.info(f"AIè«‹æ±‚å¯©è¨ˆ: {json.dumps(audit_entry)}")
    
    def _generate_request_id(self) -> str:
        """ç”Ÿæˆè«‹æ±‚ID"""
        import uuid
        return str(uuid.uuid4())
    
    def _check_user_permissions(self, user_id: str, context: Dict) -> bool:
        """æª¢æŸ¥ç”¨æˆ¶æ¬Šé™"""
        # å¯¦éš›æ‡‰ç”¨ä¸­æœƒæª¢æŸ¥Azure ADæ¬Šé™
        return True
    
    def _check_content_policy(self, text: str) -> List[Dict]:
        """æª¢æŸ¥å…§å®¹æ”¿ç­–"""
        violations = []
        
        # æª¢æŸ¥ç¦ç”¨è©å½™
        prohibited_words = ["å¯†ç¢¼", "æ©Ÿå¯†", "internal"]
        for word in prohibited_words:
            if word.lower() in text.lower():
                violations.append({
                    "type": "PROHIBITED_CONTENT",
                    "details": f"åŒ…å«ç¦ç”¨è©å½™: {word}"
                })
        
        return violations

# ä½¿ç”¨ç¯„ä¾‹
async def main():
    config = {
        "monitor_connection_string": "InstrumentationKey=your-key",
        "debug": False
    }
    
    service = SecureAzureAIService(
        key_vault_url="https://your-vault.vault.azure.net/",
        config=config
    )
    
    result = await service.secure_ai_request(
        prompt="è«‹å¹«æˆ‘åˆ†æQ3å­£åº¦çš„éŠ·å”®å ±å‘Šï¼Œæ‰¾å‡ºæ”¹é€²æ©Ÿæœƒ",
        user_id="user@company.com",
        request_context={
            "compliance_level": "HIGH",
            "business_domain": "SALES_ANALYSIS",
            "add_disclaimer": True
        }
    )
    
    print(f"AIå›æ‡‰: {result}")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸ¤– æ–°èˆˆLLMæä¾›å•†ç”Ÿæ…‹

### ğŸŒŸ Cohereï¼šä¼æ¥­ç´šRAGå°ˆå®¶

> ğŸ’¡ **ç™½è©±è§£é‡‹**  
> **ä»€éº¼æ˜¯RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰ï¼Ÿ** æƒ³åƒæ‚¨æ˜¯ä¸€å€‹å¾ˆè°æ˜ä½†è¨˜æ†¶æœ‰é™çš„é¡§å•ï¼Œå®¢æˆ¶å•æ‚¨ä¸€å€‹å°ˆæ¥­å•é¡Œæ™‚ï¼Œæ‚¨æœƒå…ˆå»æŸ¥é–±æœ€æ–°çš„è³‡æ–™å’Œæ–‡ä»¶ï¼Œç„¶å¾Œçµåˆæ‚¨çš„å°ˆæ¥­çŸ¥è­˜çµ¦å‡ºå›ç­”ã€‚RAGå°±æ˜¯é€™å€‹æ¦‚å¿µï¼å®ƒè®“AIä¸åªä¾è³´è¨“ç·´æ™‚çš„çŸ¥è­˜ï¼Œé‚„èƒ½å³æ™‚æŸ¥è©¢æœ€æ–°ã€æœ€ç›¸é—œçš„è³‡æ–™ï¼Œç„¶å¾Œæ•´åˆé€™äº›è³‡è¨Šçµ¦å‡ºæ›´æº–ç¢ºã€æ›´ç¬¦åˆå¯¦éš›æƒ…æ³çš„å›ç­”ã€‚Cohereåœ¨é€™æ–¹é¢ç‰¹åˆ¥æ“…é•·ï¼Œå°±åƒå°ˆæ¥­çš„ç ”ç©¶åŠ©ç†ä¸€æ¨£ï¼

#### ğŸ” Cohere RAGä¼æ¥­å¯¦æ–½

<div style="background-color: #F1F8E9; padding: 20px; border-left: 4px solid #8BC34A; margin: 20px 0;">

**ä¼æ¥­çŸ¥è­˜ç®¡ç†RAGç³»çµ±**
```python
import cohere
import asyncio
from typing import List, Dict, Optional
import chromadb
from sentence_transformers import SentenceTransformer
import logging
from datetime import datetime
import json

class CohereEnterpriseRAG:
    """Cohereä¼æ¥­ç´šRAGç³»çµ±"""
    
    def __init__(self, cohere_api_key: str, knowledge_base_path: str):
        self.cohere_client = cohere.Client(cohere_api_key)
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.vector_db = chromadb.Client()
        self.collection = self.vector_db.create_collection("enterprise_knowledge")
        self.knowledge_base_path = knowledge_base_path
        self.setup_logging()
        
    def setup_logging(self):
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    async def ingest_documents(self, documents: List[Dict]):
        """æ”å–ä¼æ¥­æ–‡æª”åˆ°å‘é‡è³‡æ–™åº«"""
        
        self.logger.info(f"é–‹å§‹æ”å– {len(documents)} å€‹æ–‡æª”")
        
        for doc in documents:
            try:
                # æ–‡æª”åˆ†å‰²
                chunks = self._chunk_document(doc['content'])
                
                # ç”Ÿæˆå‘é‡åµŒå…¥
                embeddings = self.embedding_model.encode(chunks)
                
                # æº–å‚™å…ƒæ•¸æ“š
                metadatas = [
                    {
                        "doc_id": doc['id'],
                        "title": doc.get('title', ''),
                        "department": doc.get('department', ''),
                        "last_updated": doc.get('last_updated', ''),
                        "chunk_index": i,
                        "security_level": doc.get('security_level', 'public')
                    }
                    for i in range(len(chunks))
                ]
                
                # å„²å­˜åˆ°å‘é‡è³‡æ–™åº«
                self.collection.add(
                    embeddings=embeddings.tolist(),
                    documents=chunks,
                    metadatas=metadatas,
                    ids=[f"{doc['id']}_chunk_{i}" for i in range(len(chunks))]
                )
                
                self.logger.info(f"æ–‡æª” {doc['id']} æ”å–å®Œæˆï¼Œåˆ†å‰²ç‚º {len(chunks)} å€‹ç‰‡æ®µ")
                
            except Exception as e:
                self.logger.error(f"æ–‡æª” {doc['id']} æ”å–å¤±æ•—: {str(e)}")
    
    def _chunk_document(self, content: str, chunk_size: int = 500) -> List[str]:
        """å°‡æ–‡æª”åˆ†å‰²ç‚ºè¼ƒå°çš„ç‰‡æ®µ"""
        sentences = content.split('ã€‚')
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) < chunk_size:
                current_chunk += sentence + "ã€‚"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    async def rag_query(
        self, 
        query: str, 
        user_context: Dict,
        max_retrieved_docs: int = 5
    ) -> Dict:
        """åŸ·è¡ŒRAGæŸ¥è©¢"""
        
        start_time = datetime.now()
        
        try:
            # 1. æª¢ç´¢ç›¸é—œæ–‡æª”
            retrieved_docs = await self._retrieve_relevant_documents(
                query, user_context, max_retrieved_docs
            )
            
            if not retrieved_docs:
                return {
                    "response": "æŠ±æ­‰ï¼Œæˆ‘æ‰¾ä¸åˆ°ç›¸é—œçš„è³‡è¨Šä¾†å›ç­”æ‚¨çš„å•é¡Œã€‚",
                    "confidence": 0.0,
                    "sources": [],
                    "processing_time": (datetime.now() - start_time).total_seconds()
                }
            
            # 2. å»ºæ§‹å¢å¼·æç¤º
            enhanced_prompt = self._build_rag_prompt(query, retrieved_docs, user_context)
            
            # 3. ä½¿ç”¨Cohereç”Ÿæˆå›æ‡‰
            response = self.cohere_client.generate(
                model='command-xlarge',
                prompt=enhanced_prompt,
                max_tokens=500,
                temperature=0.3,
                k=0,
                stop_sequences=["--END--"],
                return_likelihoods='GENERATION'
            )
            
            # 4. å¾Œè™•ç†å’Œä¿¡å¿ƒåº¦è©•ä¼°
            confidence_score = self._calculate_confidence(
                response, retrieved_docs, query
            )
            
            # 5. æº–å‚™å›æ‡‰
            result = {
                "response": response.generations[0].text.strip(),
                "confidence": confidence_score,
                "sources": [
                    {
                        "doc_id": doc["metadata"]["doc_id"],
                        "title": doc["metadata"]["title"],
                        "department": doc["metadata"]["department"],
                        "relevance_score": doc["distance"],
                        "excerpt": doc["document"][:200] + "..."
                    }
                    for doc in retrieved_docs
                ],
                "processing_time": (datetime.now() - start_time).total_seconds(),
                "retrieved_count": len(retrieved_docs)
            }
            
            self.logger.info(f"RAGæŸ¥è©¢å®Œæˆï¼Œä¿¡å¿ƒåº¦: {confidence_score:.2f}")
            return result
            
        except Exception as e:
            self.logger.error(f"RAGæŸ¥è©¢å¤±æ•—: {str(e)}")
            return {
                "error": str(e),
                "processing_time": (datetime.now() - start_time).total_seconds()
            }
    
    async def _retrieve_relevant_documents(
        self, 
        query: str, 
        user_context: Dict,
        max_docs: int
    ) -> List[Dict]:
        """æª¢ç´¢ç›¸é—œæ–‡æª”"""
        
        # ç”ŸæˆæŸ¥è©¢å‘é‡
        query_embedding = self.embedding_model.encode([query])
        
        # å»ºæ§‹æŸ¥è©¢éæ¿¾å™¨
        where_filter = {}
        if user_context.get("department"):
            where_filter["department"] = user_context["department"]
        if user_context.get("security_level"):
            where_filter["security_level"] = {"$in": ["public", user_context["security_level"]]}
        
        # åŸ·è¡Œå‘é‡ç›¸ä¼¼æ€§æœå°‹
        results = self.collection.query(
            query_embeddings=query_embedding.tolist(),
            n_results=max_docs,
            where=where_filter if where_filter else None
        )
        
        # æ ¼å¼åŒ–çµæœ
        retrieved_docs = []
        for i in range(len(results['documents'][0])):
            retrieved_docs.append({
                "document": results['documents'][0][i],
                "metadata": results['metadatas'][0][i],
                "distance": results['distances'][0][i]
            })
        
        return retrieved_docs
    
    def _build_rag_prompt(
        self, 
        query: str, 
        retrieved_docs: List[Dict],
        user_context: Dict
    ) -> str:
        """å»ºæ§‹RAGæç¤º"""
        
        context_docs = "\n\n".join([
            f"æ–‡æª”ä¾†æº: {doc['metadata']['title']} (éƒ¨é–€: {doc['metadata']['department']})\n"
            f"å…§å®¹: {doc['document']}"
            for doc in retrieved_docs
        ])
        
        prompt = f"""åŸºæ–¼ä»¥ä¸‹ä¼æ¥­å…§éƒ¨æ–‡æª”å›ç­”å•é¡Œï¼š

ç›¸é—œæ–‡æª”å…§å®¹ï¼š
{context_docs}

ç”¨æˆ¶å•é¡Œï¼š{query}

ç”¨æˆ¶èƒŒæ™¯ï¼š
- éƒ¨é–€ï¼š{user_context.get('department', 'æœªæŒ‡å®š')}
- æ¬Šé™ç­‰ç´šï¼š{user_context.get('security_level', 'standard')}

å›ç­”è¦æ±‚ï¼š
1. è«‹åŸºæ–¼ä¸Šè¿°æ–‡æª”å…§å®¹å›ç­”å•é¡Œ
2. å¦‚æœæ–‡æª”ä¸­æ²’æœ‰è¶³å¤ è³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜
3. å¼•ç”¨å…·é«”çš„æ–‡æª”ä¾†æºæ”¯æ’æ‚¨çš„å›ç­”
4. æä¾›å¯¦ç”¨çš„å»ºè­°æˆ–ä¸‹ä¸€æ­¥è¡Œå‹•
5. ä¿æŒå›ç­”çš„æº–ç¢ºæ€§å’Œå®¢è§€æ€§

å›ç­”ï¼š"""
        
        return prompt
    
    def _calculate_confidence(
        self, 
        response, 
        retrieved_docs: List[Dict], 
        query: str
    ) -> float:
        """è¨ˆç®—å›æ‡‰ä¿¡å¿ƒåº¦"""
        
        # åŸºæ–¼å¤šå€‹å› ç´ è¨ˆç®—ä¿¡å¿ƒåº¦
        factors = []
        
        # 1. æª¢ç´¢æ–‡æª”çš„ç›¸é—œæ€§
        if retrieved_docs:
            avg_distance = sum(doc["distance"] for doc in retrieved_docs) / len(retrieved_docs)
            relevance_score = max(0, 1 - avg_distance)
            factors.append(relevance_score)
        
        # 2. ç”Ÿæˆå›æ‡‰çš„ä¼¼ç„¶åº¦
        if hasattr(response, 'generations') and response.generations:
            # Cohereæä¾›çš„ç”Ÿæˆä¼¼ç„¶åº¦
            likelihood_score = 0.8  # ç°¡åŒ–è¨ˆç®—
            factors.append(likelihood_score)
        
        # 3. å›æ‡‰é•·åº¦å’Œå®Œæ•´æ€§
        response_text = response.generations[0].text if response.generations else ""
        completeness_score = min(1.0, len(response_text) / 200)
        factors.append(completeness_score)
        
        # åŠ æ¬Šå¹³å‡
        if factors:
            return sum(factors) / len(factors)
        else:
            return 0.0

# ä½¿ç”¨ç¯„ä¾‹
async def main():
    # åˆå§‹åŒ–RAGç³»çµ±
    rag_system = CohereEnterpriseRAG(
        cohere_api_key="your-cohere-api-key",
        knowledge_base_path="./enterprise_docs"
    )
    
    # æ”å–ä¼æ¥­æ–‡æª”
    sample_documents = [
        {
            "id": "hr_001",
            "title": "å“¡å·¥æ‰‹å†Š - è«‹å‡æ”¿ç­–",
            "content": "å“¡å·¥è«‹å‡éœ€è¦æå‰3å¤©ç”³è«‹ï¼Œç—…å‡éœ€è¦é†«ç”Ÿè­‰æ˜...",
            "department": "human_resources",
            "security_level": "internal",
            "last_updated": "2024-01-15"
        },
        {
            "id": "it_001", 
            "title": "è³‡å®‰æ”¿ç­–æŒ‡å—",
            "content": "å…¬å¸è³‡å®‰æ”¿ç­–è¦æ±‚æ‰€æœ‰å“¡å·¥ä½¿ç”¨é›™å› å­èªè­‰...",
            "department": "information_technology",
            "security_level": "confidential",
            "last_updated": "2024-02-01"
        }
    ]
    
    await rag_system.ingest_documents(sample_documents)
    
    # åŸ·è¡ŒRAGæŸ¥è©¢
    result = await rag_system.rag_query(
        query="è«‹å‡æ”¿ç­–æ˜¯ä»€éº¼ï¼Ÿéœ€è¦ä»€éº¼æ–‡ä»¶ï¼Ÿ",
        user_context={
            "department": "sales",
            "security_level": "internal"
        }
    )
    
    print(f"å›ç­”: {result['response']}")
    print(f"ä¿¡å¿ƒåº¦: {result['confidence']:.2f}")
    print(f"åƒè€ƒä¾†æº: {len(result['sources'])} å€‹æ–‡æª”")

if __name__ == "__main__":
    asyncio.run(main())
```

</div>

### ğŸ¤— Hugging Faceï¼šé–‹æºå‰µæ–°å¼•æ“

#### ğŸ› ï¸ ä¼æ¥­ç´šHugging Face Hubæ•´åˆ

**æ¨¡å‹ç®¡ç†èˆ‡éƒ¨ç½²å¹³å°**
```python
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    AutoModelForSequenceClassification, pipeline
)
from huggingface_hub import HfApi, Repository
import torch
import asyncio
from typing import Dict, List, Optional
import logging
from datetime import datetime
import json
import os

class HuggingFaceEnterpriseHub:
    """ä¼æ¥­ç´šHugging Faceæ¨¡å‹ç®¡ç†å¹³å°"""
    
    def __init__(self, hf_token: str, organization: str):
        self.hf_token = hf_token
        self.organization = organization
        self.api = HfApi(token=hf_token)
        self.loaded_models = {}
        self.setup_logging()
        
    def setup_logging(self):
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    async def deploy_custom_model(
        self, 
        model_name: str,
        base_model: str,
        training_data: List[Dict],
        task_type: str = "text-generation"
    ) -> Dict:
        """éƒ¨ç½²å®¢è£½åŒ–æ¨¡å‹"""
        
        start_time = datetime.now()
        
        try:
            # 1. è¼‰å…¥åŸºç¤æ¨¡å‹
            self.logger.info(f"è¼‰å…¥åŸºç¤æ¨¡å‹: {base_model}")
            
            tokenizer = AutoTokenizer.from_pretrained(base_model)
            if task_type == "text-generation":
                model = AutoModelForCausalLM.from_pretrained(
                    base_model,
                    torch_dtype=torch.float16,
                    device_map="auto"
                )
            elif task_type == "classification":
                model = AutoModelForSequenceClassification.from_pretrained(
                    base_model,
                    num_labels=len(set(item['label'] for item in training_data))
                )
            
            # 2. æº–å‚™è¨“ç·´è³‡æ–™
            processed_data = self._prepare_training_data(
                training_data, tokenizer, task_type
            )
            
            # 3. å¾®èª¿æ¨¡å‹
            fine_tuned_model = await self._fine_tune_model(
                model, tokenizer, processed_data, task_type
            )
            
            # 4. è©•ä¼°æ¨¡å‹æ•ˆèƒ½
            evaluation_results = await self._evaluate_model(
                fine_tuned_model, tokenizer, processed_data
            )
            
            # 5. ä¸Šå‚³åˆ°Hugging Face Hub
            model_id = f"{self.organization}/{model_name}"
            self._upload_to_hub(fine_tuned_model, tokenizer, model_id)
            
            # 6. è¨»å†Šåˆ°ä¼æ¥­æ¨¡å‹åº«
            registration_info = {
                "model_id": model_id,
                "base_model": base_model,
                "task_type": task_type,
                "training_samples": len(training_data),
                "evaluation_results": evaluation_results,
                "created_at": datetime.now().isoformat(),
                "created_by": self.organization
            }
            
            self._register_enterprise_model(registration_info)
            
            deployment_time = (datetime.now() - start_time).total_seconds()
            
            return {
                "success": True,
                "model_id": model_id,
                "evaluation_results": evaluation_results,
                "deployment_time": deployment_time,
                "model_size_mb": self._calculate_model_size(fine_tuned_model),
                "next_steps": [
                    "æ¨¡å‹å·²éƒ¨ç½²åˆ°ä¼æ¥­Hub",
                    "å¯é€šéAPIé€²è¡Œæ¨ç†",
                    "å»ºè­°é€²è¡ŒA/Bæ¸¬è©¦é©—è­‰æ•ˆæœ"
                ]
            }
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹éƒ¨ç½²å¤±æ•—: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "deployment_time": (datetime.now() - start_time).total_seconds()
            }
    
    def _prepare_training_data(
        self, 
        data: List[Dict], 
        tokenizer, 
        task_type: str
    ) -> Dict:
        """æº–å‚™è¨“ç·´è³‡æ–™"""
        
        if task_type == "text-generation":
            # æº–å‚™ç”Ÿæˆä»»å‹™è³‡æ–™
            texts = []
            for item in data:
                formatted_text = f"å•é¡Œ: {item['input']}\nå›ç­”: {item['output']}<|endoftext|>"
                texts.append(formatted_text)
            
            encodings = tokenizer(
                texts,
                truncation=True,
                padding=True,
                max_length=512,
                return_tensors="pt"
            )
            
            return {
                "input_ids": encodings["input_ids"],
                "attention_mask": encodings["attention_mask"],
                "labels": encodings["input_ids"].clone()
            }
            
        elif task_type == "classification":
            # æº–å‚™åˆ†é¡ä»»å‹™è³‡æ–™
            texts = [item['text'] for item in data]
            labels = [item['label'] for item in data]
            
            # å»ºç«‹æ¨™ç±¤å°æ˜ 
            unique_labels = list(set(labels))
            label_to_id = {label: idx for idx, label in enumerate(unique_labels)}
            label_ids = [label_to_id[label] for label in labels]
            
            encodings = tokenizer(
                texts,
                truncation=True,
                padding=True,
                max_length=512,
                return_tensors="pt"
            )
            
            return {
                "input_ids": encodings["input_ids"],
                "attention_mask": encodings["attention_mask"],
                "labels": torch.tensor(label_ids),
                "label_mapping": label_to_id
            }
    
    async def _fine_tune_model(
        self, 
        model, 
        tokenizer, 
        data: Dict, 
        task_type: str
    ):
        """å¾®èª¿æ¨¡å‹"""
        
        from transformers import Trainer, TrainingArguments
        from torch.utils.data import Dataset
        
        class CustomDataset(Dataset):
            def __init__(self, encodings):
                self.encodings = encodings
            
            def __getitem__(self, idx):
                return {key: val[idx] for key, val in self.encodings.items()}
            
            def __len__(self):
                return len(self.encodings['input_ids'])
        
        # å»ºç«‹è³‡æ–™é›†
        dataset = CustomDataset(data)
        
        # è¨­å®šè¨“ç·´åƒæ•¸
        training_args = TrainingArguments(
            output_dir='./tmp_trainer',
            num_train_epochs=3,
            per_device_train_batch_size=4,
            per_device_eval_batch_size=4,
            warmup_steps=100,
            weight_decay=0.01,
            logging_dir='./logs',
            logging_steps=50,
            save_steps=500,
            evaluation_strategy="steps",
            eval_steps=500,
            load_best_model_at_end=True,
        )
        
        # å»ºç«‹è¨“ç·´å™¨
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=dataset,
            eval_dataset=dataset,  # å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰è©²åˆ†é›¢é©—è­‰é›†
            tokenizer=tokenizer,
        )
        
        # é–‹å§‹å¾®èª¿
        self.logger.info("é–‹å§‹æ¨¡å‹å¾®èª¿...")
        trainer.train()
        
        return model
    
    async def _evaluate_model(self, model, tokenizer, data: Dict) -> Dict:
        """è©•ä¼°æ¨¡å‹æ•ˆèƒ½"""
        
        evaluation_results = {
            "accuracy": 0.0,
            "loss": 0.0,
            "inference_time_ms": 0.0,
            "sample_predictions": []
        }
        
        model.eval()
        with torch.no_grad():
            # ç°¡åŒ–çš„è©•ä¼°éç¨‹
            sample_inputs = data["input_ids"][:5]  # å–5å€‹æ¨£æœ¬
            
            start_time = datetime.now()
            outputs = model(sample_inputs)
            inference_time = (datetime.now() - start_time).total_seconds() * 1000
            
            evaluation_results["inference_time_ms"] = inference_time / len(sample_inputs)
        
        return evaluation_results
    
    def _upload_to_hub(self, model, tokenizer, model_id: str):
        """ä¸Šå‚³æ¨¡å‹åˆ°Hugging Face Hub"""
        
        try:
            # ä¸Šå‚³æ¨¡å‹
            model.push_to_hub(model_id, token=self.hf_token)
            tokenizer.push_to_hub(model_id, token=self.hf_token)
            
            # å»ºç«‹æ¨¡å‹å¡ç‰‡
            model_card = f"""
---
language: 
- zh
- en
library_name: transformers
pipeline_tag: text-generation
tags:
- enterprise
- custom-trained
- {self.organization}
---

# {model_id}

é€™æ˜¯ {self.organization} çµ„ç¹”è¨“ç·´çš„ä¼æ¥­ç´šæ¨¡å‹ã€‚

## ä½¿ç”¨æ–¹å¼

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("{model_id}")
model = AutoModelForCausalLM.from_pretrained("{model_id}")

# ç”Ÿæˆæ–‡æœ¬
inputs = tokenizer("æ‚¨çš„æç¤º", return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## æ³¨æ„äº‹é …

- æ­¤æ¨¡å‹å°ˆç‚ºä¼æ¥­å…§éƒ¨ä½¿ç”¨è€Œè¨­è¨ˆ
- è«‹éµå¾ªå…¬å¸çš„AIä½¿ç”¨æ”¿ç­–
- å¦‚æœ‰å•é¡Œè«‹è¯ç¹«ITéƒ¨é–€
            """
            
            # ä¸Šå‚³æ¨¡å‹å¡ç‰‡
            self.api.upload_file(
                path_or_fileobj=model_card.encode(),
                path_in_repo="README.md",
                repo_id=model_id,
                token=self.hf_token
            )
            
            self.logger.info(f"æ¨¡å‹æˆåŠŸä¸Šå‚³åˆ°Hub: {model_id}")
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹ä¸Šå‚³å¤±æ•—: {str(e)}")
            raise
    
    def _register_enterprise_model(self, registration_info: Dict):
        """è¨»å†Šåˆ°ä¼æ¥­æ¨¡å‹åº«"""
        
        # å¯«å…¥ä¼æ¥­æ¨¡å‹è¨»å†Šè¡¨
        registry_file = "enterprise_model_registry.json"
        
        if os.path.exists(registry_file):
            with open(registry_file, 'r', encoding='utf-8') as f:
                registry = json.load(f)
        else:
            registry = {"models": []}
        
        registry["models"].append(registration_info)
        
        with open(registry_file, 'w', encoding='utf-8') as f:
            json.dump(registry, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"æ¨¡å‹å·²è¨»å†Šåˆ°ä¼æ¥­åº«: {registration_info['model_id']}")
    
    def _calculate_model_size(self, model) -> float:
        """è¨ˆç®—æ¨¡å‹å¤§å°ï¼ˆMBï¼‰"""
        
        param_count = sum(p.numel() for p in model.parameters())
        # å‡è¨­æ¯å€‹åƒæ•¸4 bytes (float32)
        size_mb = param_count * 4 / (1024 * 1024)
        return round(size_mb, 2)
    
    async def create_inference_pipeline(
        self, 
        model_id: str, 
        task_type: str = "text-generation"
    ):
        """å»ºç«‹æ¨ç†ç®¡é“"""
        
        if model_id not in self.loaded_models:
            self.logger.info(f"è¼‰å…¥æ¨¡å‹: {model_id}")
            
            pipe = pipeline(
                task_type,
                model=model_id,
                tokenizer=model_id,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            
            self.loaded_models[model_id] = pipe
        
        return self.loaded_models[model_id]
    
    async def batch_inference(
        self, 
        model_id: str, 
        inputs: List[str],
        **generation_kwargs
    ) -> List[Dict]:
        """æ‰¹é‡æ¨ç†"""
        
        pipe = await self.create_inference_pipeline(model_id)
        
        results = []
        for input_text in inputs:
            try:
                output = pipe(input_text, **generation_kwargs)
                results.append({
                    "input": input_text,
                    "output": output,
                    "success": True
                })
            except Exception as e:
                results.append({
                    "input": input_text,
                    "error": str(e),
                    "success": False
                })
        
        return results

# ä½¿ç”¨ç¯„ä¾‹
async def main():
    # åˆå§‹åŒ–ä¼æ¥­Hub
    enterprise_hub = HuggingFaceEnterpriseHub(
        hf_token="your-huggingface-token",
        organization="your-company"
    )
    
    # æº–å‚™è¨“ç·´è³‡æ–™
    training_data = [
        {
            "input": "å¦‚ä½•ç”³è«‹å¹´å‡ï¼Ÿ",
            "output": "è«‹ç™»å…¥äººäº‹ç³»çµ±å¡«å¯«å¹´å‡ç”³è«‹è¡¨ï¼Œä¸¦æå‰7å¤©æäº¤çµ¦ç›´å±¬ä¸»ç®¡å¯©æ ¸ã€‚"
        },
        {
            "input": "å…¬å¸çš„åŠ ç­æ”¿ç­–æ˜¯ä»€éº¼ï¼Ÿ",
            "output": "åŠ ç­éœ€è¦äº‹å‰ç”³è«‹ï¼Œè¶…éæ™šä¸Š9é»çš„åŠ ç­éœ€è¦éƒ¨é–€ä¸»ç®¡ç‰¹åˆ¥æ‰¹å‡†ã€‚"
        }
        # æ›´å¤šè¨“ç·´è³‡æ–™...
    ]
    
    # éƒ¨ç½²å®¢è£½åŒ–æ¨¡å‹
    deployment_result = await enterprise_hub.deploy_custom_model(
        model_name="company-hr-assistant",
        base_model="microsoft/DialoGPT-medium",
        training_data=training_data,
        task_type="text-generation"
    )
    
    if deployment_result["success"]:
        print(f"æ¨¡å‹éƒ¨ç½²æˆåŠŸ: {deployment_result['model_id']}")
        
        # æ¸¬è©¦æ‰¹é‡æ¨ç†
        test_inputs = [
            "è«‹å‡æµç¨‹æ˜¯ä»€éº¼ï¼Ÿ",
            "å¦‚ä½•ç”³è«‹å‡ºå·®ï¼Ÿ"
        ]
        
        inference_results = await enterprise_hub.batch_inference(
            model_id=deployment_result['model_id'],
            inputs=test_inputs,
            max_length=100,
            temperature=0.7
        )
        
        for result in inference_results:
            print(f"å•é¡Œ: {result['input']}")
            print(f"å›ç­”: {result['output']}")
            print("---")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸ”— è·¨å¹³å°æ•´åˆæ¶æ§‹

### ğŸ¯ æ™ºèƒ½ç·¨æ’ç³»çµ±

> ğŸ’¡ **ç™½è©±è§£é‡‹**  
> **ä»€éº¼æ˜¯AIç·¨æ’ç³»çµ±ï¼Ÿ** æƒ³åƒæ‚¨æ˜¯ä¸€å€‹å¤§å‹é†«é™¢çš„ç¸½æŒ‡æ®ï¼Œæœ‰å¿ƒè‡Ÿç§‘ã€è…¦ç§‘ã€éª¨ç§‘ç­‰ä¸åŒå°ˆç§‘é†«ç”Ÿã€‚ç•¶ç—…äººä¾†çœ‹ç—…æ™‚ï¼Œæ‚¨éœ€è¦æ ¹æ“šç—…æƒ…æ±ºå®šæ´¾å“ªå€‹é†«ç”Ÿæœ€åˆé©ï¼Œæœ‰æ™‚å€™é‚„éœ€è¦å¤šå€‹å°ˆç§‘é†«ç”Ÿæœƒè¨ºã€‚AIç·¨æ’ç³»çµ±å°±æ˜¯é€™æ¨£çš„ã€Œç¸½æŒ‡æ®ã€ï¼Œå®ƒæœƒæ ¹æ“šç”¨æˆ¶çš„å•é¡Œé¡å‹ã€è¤‡é›œåº¦ã€æˆæœ¬è€ƒé‡ç­‰å› ç´ ï¼Œè‡ªå‹•æ±ºå®šè¦ç”¨å“ªå€‹AIå¹³å°ï¼Œæˆ–æ˜¯è®“å¤šå€‹AIå¹³å°ä¸€èµ·å·¥ä½œï¼Œç¢ºä¿æ¯å€‹å•é¡Œéƒ½èƒ½å¾—åˆ°æœ€é©åˆã€æœ€é«˜å“è³ªçš„å›ç­”ï¼

<div style="background-color: #E1F5FE; padding: 20px; border-left: 4px solid #03A9F4; margin: 20px 0;">

**ä¼æ¥­ç´šAIç·¨æ’ç³»çµ±æ¶æ§‹**
```python
import asyncio
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum
import json
from datetime import datetime
import aiohttp
import hashlib

class TaskComplexity(Enum):
    SIMPLE = "simple"
    MEDIUM = "medium"
    COMPLEX = "complex"
    CRITICAL = "critical"

class ProviderCapability(Enum):
    TEXT_GENERATION = "text_generation"
    CODE_GENERATION = "code_generation"
    ANALYSIS = "analysis"
    CREATIVE_WRITING = "creative_writing"
    MULTILINGUAL = "multilingual"
    MULTIMODAL = "multimodal"
    RAG = "rag"

@dataclass
class TaskRequest:
    task_id: str
    user_id: str
    prompt: str
    task_type: str
    complexity: TaskComplexity
    requirements: Dict[str, Any]
    constraints: Dict[str, Any]
    context: Dict[str, Any]

class AIProviderOrchestrator:
    """AIæä¾›å•†æ™ºèƒ½ç·¨æ’ç³»çµ±"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.providers = self._initialize_providers()
        self.routing_rules = self._load_routing_rules()
        self.performance_metrics = {}
        self.setup_logging()
        
    def setup_logging(self):
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def _initialize_providers(self) -> Dict:
        """åˆå§‹åŒ–æ‰€æœ‰AIæä¾›å•†"""
        
        providers = {
            "openai": {
                "capabilities": [
                    ProviderCapability.TEXT_GENERATION,
                    ProviderCapability.CODE_GENERATION,
                    ProviderCapability.CREATIVE_WRITING
                ],
                "cost_per_1k_tokens": 0.002,
                "avg_response_time": 1.2,
                "reliability_score": 0.98,
                "max_context_length": 16000,
                "supported_languages": ["en", "zh", "ja", "ko"],
                "api_endpoint": self.config.get("openai_endpoint"),
                "api_key": self.config.get("openai_api_key")
            },
            
            "anthropic": {
                "capabilities": [
                    ProviderCapability.TEXT_GENERATION,
                    ProviderCapability.ANALYSIS,
                    ProviderCapability.CODE_GENERATION
                ],
                "cost_per_1k_tokens": 0.008,
                "avg_response_time": 1.8,
                "reliability_score": 0.99,
                "max_context_length": 100000,
                "supported_languages": ["en", "zh"],
                "api_endpoint": self.config.get("anthropic_endpoint"),
                "api_key": self.config.get("anthropic_api_key")
            },
            
            "cohere": {
                "capabilities": [
                    ProviderCapability.TEXT_GENERATION,
                    ProviderCapability.MULTILINGUAL,
                    ProviderCapability.RAG
                ],
                "cost_per_1k_tokens": 0.001,
                "avg_response_time": 0.9,
                "reliability_score": 0.96,
                "max_context_length": 4000,
                "supported_languages": ["en", "zh", "ja", "ko", "es", "fr"],
                "api_endpoint": self.config.get("cohere_endpoint"),
                "api_key": self.config.get("cohere_api_key")
            },
            
            "azure": {
                "capabilities": [
                    ProviderCapability.TEXT_GENERATION,
                    ProviderCapability.MULTIMODAL,
                    ProviderCapability.ANALYSIS
                ],
                "cost_per_1k_tokens": 0.002,
                "avg_response_time": 1.5,
                "reliability_score": 0.99,
                "max_context_length": 32000,
                "supported_languages": ["en", "zh"],
                "api_endpoint": self.config.get("azure_endpoint"),
                "api_key": self.config.get("azure_api_key"),
                "compliance_level": "enterprise"
            },
            
            "llama": {
                "capabilities": [
                    ProviderCapability.TEXT_GENERATION,
                    ProviderCapability.CODE_GENERATION,
                    ProviderCapability.MULTILINGUAL
                ],
                "cost_per_1k_tokens": 0.0,  # è‡ªå»ºç„¡APIè²»ç”¨
                "avg_response_time": 2.1,
                "reliability_score": 0.94,
                "max_context_length": 8000,
                "supported_languages": ["en", "zh", "es", "fr"],
                "api_endpoint": self.config.get("llama_endpoint"),
                "deployment_type": "self_hosted"
            }
        }
        
        return providers
    
    def _load_routing_rules(self) -> Dict:
        """è¼‰å…¥è·¯ç”±è¦å‰‡"""
        
        return {
            "creative_writing": {
                "preferred_providers": ["openai", "anthropic"],
                "fallback_providers": ["cohere", "llama"],
                "min_quality_score": 0.8
            },
            
            "business_analysis": {
                "preferred_providers": ["anthropic", "azure"],
                "fallback_providers": ["openai", "cohere"],
                "min_quality_score": 0.9,
                "requires_compliance": True
            },
            
            "code_generation": {
                "preferred_providers": ["openai", "llama"],
                "fallback_providers": ["anthropic"],
                "min_quality_score": 0.85
            },
            
            "multilingual_tasks": {
                "preferred_providers": ["cohere", "llama"],
                "fallback_providers": ["openai"],
                "min_quality_score": 0.8
            },
            
            "cost_sensitive": {
                "preferred_providers": ["llama", "cohere"],
                "fallback_providers": ["openai"],
                "max_cost_per_request": 0.01
            },
            
            "high_reliability": {
                "preferred_providers": ["azure", "anthropic"],
                "fallback_providers": ["openai"],
                "min_reliability_score": 0.98
            }
        }
    
    async def route_request(self, request: TaskRequest) -> Dict:
        """æ™ºèƒ½è·¯ç”±è«‹æ±‚åˆ°æœ€é©åˆçš„æä¾›å•†"""
        
        start_time = datetime.now()
        
        try:
            # 1. åˆ†æè«‹æ±‚ç‰¹æ€§
            request_analysis = self._analyze_request(request)
            
            # 2. é¸æ“‡å€™é¸æä¾›å•†
            candidates = self._select_candidates(request, request_analysis)
            
            # 3. è©•ä¼°å’Œæ’åºå€™é¸è€…
            ranked_providers = self._rank_providers(candidates, request, request_analysis)
            
            # 4. åŸ·è¡Œè«‹æ±‚ï¼ˆå«æ•…éšœè½‰ç§»ï¼‰
            result = await self._execute_with_fallback(request, ranked_providers)
            
            # 5. è¨˜éŒ„æ•ˆèƒ½æŒ‡æ¨™
            self._update_performance_metrics(
                result["provider_used"], 
                request, 
                result["response_time"],
                result["success"]
            )
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return {
                **result,
                "routing_time": processing_time,
                "request_analysis": request_analysis,
                "considered_providers": [p["name"] for p in ranked_providers]
            }
            
        except Exception as e:
            self.logger.error(f"è«‹æ±‚è·¯ç”±å¤±æ•—: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "routing_time": (datetime.now() - start_time).total_seconds()
            }
    
    def _analyze_request(self, request: TaskRequest) -> Dict:
        """åˆ†æè«‹æ±‚ç‰¹æ€§"""
        
        analysis = {
            "estimated_tokens": self._estimate_token_count(request.prompt),
            "language": self._detect_language(request.prompt),
            "domain": self._detect_domain(request.prompt),
            "sensitivity_level": self._assess_sensitivity(request.prompt),
            "required_capabilities": self._identify_capabilities(request),
            "performance_requirements": self._extract_performance_requirements(request)
        }
        
        return analysis
    
    def _select_candidates(self, request: TaskRequest, analysis: Dict) -> List[str]:
        """é¸æ“‡å€™é¸æä¾›å•†"""
        
        candidates = []
        
        # åŸºæ–¼ä»»å‹™é¡å‹é¸æ“‡
        if request.task_type in self.routing_rules:
            rule = self.routing_rules[request.task_type]
            candidates.extend(rule["preferred_providers"])
        
        # åŸºæ–¼èƒ½åŠ›éœ€æ±‚éæ¿¾
        for capability in analysis["required_capabilities"]:
            capability_providers = [
                name for name, info in self.providers.items()
                if capability in info["capabilities"]
            ]
            candidates = list(set(candidates) & set(capability_providers))
        
        # åŸºæ–¼ç´„æŸæ¢ä»¶éæ¿¾
        filtered_candidates = []
        for provider_name in candidates:
            provider = self.providers[provider_name]
            
            # æª¢æŸ¥èªè¨€æ”¯æ´
            if analysis["language"] not in provider["supported_languages"]:
                continue
            
            # æª¢æŸ¥åˆè¦è¦æ±‚
            if (request.constraints.get("requires_compliance") and 
                provider.get("compliance_level") != "enterprise"):
                continue
            
            # æª¢æŸ¥æˆæœ¬é™åˆ¶
            max_cost = request.constraints.get("max_cost_per_request", float('inf'))
            estimated_cost = self._estimate_cost(provider, analysis["estimated_tokens"])
            if estimated_cost > max_cost:
                continue
            
            filtered_candidates.append(provider_name)
        
        return filtered_candidates
    
    def _rank_providers(
        self, 
        candidates: List[str], 
        request: TaskRequest, 
        analysis: Dict
    ) -> List[Dict]:
        """è©•ä¼°å’Œæ’åºæä¾›å•†"""
        
        ranked = []
        
        for provider_name in candidates:
            provider = self.providers[provider_name]
            
            score = self._calculate_provider_score(
                provider, request, analysis
            )
            
            ranked.append({
                "name": provider_name,
                "info": provider,
                "score": score,
                "estimated_cost": self._estimate_cost(provider, analysis["estimated_tokens"]),
                "estimated_time": provider["avg_response_time"]
            })
        
        # æŒ‰åˆ†æ•¸æ’åº
        ranked.sort(key=lambda x: x["score"], reverse=True)
        
        return ranked
    
    def _calculate_provider_score(
        self, 
        provider: Dict, 
        request: TaskRequest, 
        analysis: Dict
    ) -> float:
        """è¨ˆç®—æä¾›å•†è©•åˆ†"""
        
        score = 0.0
        
        # å¯é æ€§æ¬Šé‡ (30%)
        score += provider["reliability_score"] * 0.3
        
        # æ•ˆèƒ½æ¬Šé‡ (25%)
        performance_score = 1.0 / (1.0 + provider["avg_response_time"])
        score += performance_score * 0.25
        
        # æˆæœ¬æ•ˆç›Šæ¬Šé‡ (20%)
        estimated_cost = self._estimate_cost(provider, analysis["estimated_tokens"])
        if estimated_cost == 0:
            cost_score = 1.0
        else:
            cost_score = 1.0 / (1.0 + estimated_cost)
        score += cost_score * 0.20
        
        # èƒ½åŠ›åŒ¹é…æ¬Šé‡ (15%)
        capability_match = len(
            set(analysis["required_capabilities"]) & 
            set(provider["capabilities"])
        ) / len(analysis["required_capabilities"])
        score += capability_match * 0.15
        
        # æ­·å²æ•ˆèƒ½æ¬Šé‡ (10%)
        historical_score = self.performance_metrics.get(
            provider.get("name", "unknown"), {}
        ).get("avg_quality_score", 0.8)
        score += historical_score * 0.10
        
        return score
    
    async def _execute_with_fallback(
        self, 
        request: TaskRequest, 
        ranked_providers: List[Dict]
    ) -> Dict:
        """åŸ·è¡Œè«‹æ±‚ä¸¦æ”¯æ´æ•…éšœè½‰ç§»"""
        
        last_error = None
        
        for provider_info in ranked_providers:
            try:
                provider_name = provider_info["name"]
                self.logger.info(f"å˜—è©¦ä½¿ç”¨æä¾›å•†: {provider_name}")
                
                result = await self._execute_request(request, provider_info)
                
                if result["success"]:
                    result["provider_used"] = provider_name
                    result["fallback_count"] = ranked_providers.index(provider_info)
                    return result
                else:
                    last_error = result.get("error", "æœªçŸ¥éŒ¯èª¤")
                    self.logger.warning(f"æä¾›å•† {provider_name} è«‹æ±‚å¤±æ•—: {last_error}")
                    
            except Exception as e:
                last_error = str(e)
                self.logger.error(f"æä¾›å•† {provider_name} åŸ·è¡Œç•°å¸¸: {last_error}")
        
        # æ‰€æœ‰æä¾›å•†éƒ½å¤±æ•—
        return {
            "success": False,
            "error": f"æ‰€æœ‰æä¾›å•†éƒ½å¤±æ•—ï¼Œæœ€å¾ŒéŒ¯èª¤: {last_error}",
            "provider_used": None,
            "fallback_count": len(ranked_providers)
        }
    
    async def _execute_request(self, request: TaskRequest, provider_info: Dict) -> Dict:
        """åŸ·è¡Œå…·é«”çš„AIè«‹æ±‚"""
        
        provider_name = provider_info["name"]
        provider = provider_info["info"]
        
        start_time = datetime.now()
        
        # å»ºæ§‹æä¾›å•†ç‰¹å®šçš„è«‹æ±‚
        api_request = self._build_provider_request(request, provider_name)
        
        # åŸ·è¡ŒHTTPè«‹æ±‚
        async with aiohttp.ClientSession() as session:
            headers = {
                "Authorization": f"Bearer {provider['api_key']}",
                "Content-Type": "application/json"
            }
            
            async with session.post(
                provider["api_endpoint"],
                headers=headers,
                json=api_request,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                
                response_time = (datetime.now() - start_time).total_seconds()
                
                if response.status == 200:
                    result = await response.json()
                    processed_result = self._process_provider_response(
                        result, provider_name
                    )
                    
                    return {
                        "success": True,
                        "response": processed_result,
                        "response_time": response_time,
                        "raw_response": result
                    }
                else:
                    error_text = await response.text()
                    return {
                        "success": False,
                        "error": f"HTTP {response.status}: {error_text}",
                        "response_time": response_time
                    }
    
    def _build_provider_request(self, request: TaskRequest, provider_name: str) -> Dict:
        """å»ºæ§‹æä¾›å•†ç‰¹å®šçš„è«‹æ±‚æ ¼å¼"""
        
        if provider_name == "openai":
            return {
                "model": "gpt-4",
                "messages": [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": request.prompt}
                ],
                "temperature": request.requirements.get("temperature", 0.7),
                "max_tokens": request.requirements.get("max_tokens", 500)
            }
        
        elif provider_name == "anthropic":
            return {
                "model": "claude-3-sonnet-20240229",
                "max_tokens": request.requirements.get("max_tokens", 500),
                "temperature": request.requirements.get("temperature", 0.7),
                "messages": [
                    {"role": "user", "content": request.prompt}
                ]
            }
        
        elif provider_name == "cohere":
            return {
                "model": "command-xlarge",
                "prompt": request.prompt,
                "max_tokens": request.requirements.get("max_tokens", 500),
                "temperature": request.requirements.get("temperature", 0.7)
            }
        
        # å…¶ä»–æä¾›å•†çš„æ ¼å¼...
        return {"prompt": request.prompt}
    
    def _process_provider_response(self, response: Dict, provider_name: str) -> str:
        """è™•ç†æä¾›å•†å›æ‡‰æ ¼å¼"""
        
        if provider_name == "openai":
            return response["choices"][0]["message"]["content"]
        
        elif provider_name == "anthropic":
            return response["content"][0]["text"]
        
        elif provider_name == "cohere":
            return response["generations"][0]["text"]
        
        # é è¨­è™•ç†
        return str(response)
    
    def _estimate_token_count(self, text: str) -> int:
        """ä¼°è¨ˆTokenæ•¸é‡"""
        # ç°¡åŒ–ä¼°è¨ˆï¼š1å€‹ä¸­æ–‡å­—ç¬¦ç´„ç­‰æ–¼1.5å€‹tokenï¼Œè‹±æ–‡å–®å­—ç´„1å€‹token
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        english_words = len([w for w in text.split() if w.isalpha()])
        return int(chinese_chars * 1.5 + english_words)
    
    def _detect_language(self, text: str) -> str:
        """æª¢æ¸¬æ–‡æœ¬èªè¨€"""
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        if chinese_chars > len(text) * 0.3:
            return "zh"
        return "en"
    
    def _detect_domain(self, text: str) -> str:
        """æª¢æ¸¬é ˜åŸŸé¡å‹"""
        # ç°¡åŒ–çš„é ˜åŸŸæª¢æ¸¬
        keywords = {
            "business": ["ç‡Ÿæ”¶", "åˆ©æ½¤", "å¸‚å ´", "å®¢æˆ¶", "éŠ·å”®"],
            "technology": ["ç¨‹å¼", "ç³»çµ±", "API", "è³‡æ–™åº«", "æ¼”ç®—æ³•"],
            "healthcare": ["é†«ç™‚", "ç—…æ‚£", "æ²»ç™‚", "è¨ºæ–·", "è—¥ç‰©"],
            "education": ["å­¸ç¿’", "æ•™è‚²", "èª²ç¨‹", "å­¸ç”Ÿ", "æ•™å­¸"]
        }
        
        for domain, words in keywords.items():
            if any(word in text for word in words):
                return domain
        
        return "general"
    
    def _assess_sensitivity(self, text: str) -> str:
        """è©•ä¼°æ•æ„Ÿåº¦ç­‰ç´š"""
        sensitive_keywords = ["æ©Ÿå¯†", "å…§éƒ¨", "ç§äºº", "æ•æ„Ÿ", "confidential"]
        
        if any(keyword in text.lower() for keyword in sensitive_keywords):
            return "high"
        
        return "normal"
    
    def _identify_capabilities(self, request: TaskRequest) -> List[ProviderCapability]:
        """è­˜åˆ¥æ‰€éœ€èƒ½åŠ›"""
        capabilities = []
        
        if request.task_type == "code_generation":
            capabilities.append(ProviderCapability.CODE_GENERATION)
        elif request.task_type == "creative_writing":
            capabilities.append(ProviderCapability.CREATIVE_WRITING)
        elif request.task_type == "analysis":
            capabilities.append(ProviderCapability.ANALYSIS)
        else:
            capabilities.append(ProviderCapability.TEXT_GENERATION)
        
        # æª¢æŸ¥å¤šèªè¨€éœ€æ±‚
        if self._detect_language(request.prompt) != "en":
            capabilities.append(ProviderCapability.MULTILINGUAL)
        
        return capabilities
    
    def _extract_performance_requirements(self, request: TaskRequest) -> Dict:
        """æå–æ•ˆèƒ½éœ€æ±‚"""
        return {
            "max_response_time": request.constraints.get("max_response_time", 10.0),
            "min_quality_score": request.constraints.get("min_quality_score", 0.8),
            "max_cost": request.constraints.get("max_cost_per_request", 0.1)
        }
    
    def _estimate_cost(self, provider: Dict, token_count: int) -> float:
        """ä¼°è¨ˆè«‹æ±‚æˆæœ¬"""
        cost_per_1k = provider.get("cost_per_1k_tokens", 0.0)
        return (token_count / 1000) * cost_per_1k
    
    def _update_performance_metrics(
        self, 
        provider_name: str, 
        request: TaskRequest,
        response_time: float,
        success: bool
    ):
        """æ›´æ–°æ•ˆèƒ½æŒ‡æ¨™"""
        
        if provider_name not in self.performance_metrics:
            self.performance_metrics[provider_name] = {
                "total_requests": 0,
                "successful_requests": 0,
                "total_response_time": 0.0,
                "avg_quality_score": 0.8
            }
        
        metrics = self.performance_metrics[provider_name]
        metrics["total_requests"] += 1
        metrics["total_response_time"] += response_time
        
        if success:
            metrics["successful_requests"] += 1
        
        # è¨ˆç®—å¹³å‡å›æ‡‰æ™‚é–“
        metrics["avg_response_time"] = (
            metrics["total_response_time"] / metrics["total_requests"]
        )
        
        # è¨ˆç®—æˆåŠŸç‡
        metrics["success_rate"] = (
            metrics["successful_requests"] / metrics["total_requests"]
        )

# ä½¿ç”¨ç¯„ä¾‹
async def main():
    config = {
        "openai_endpoint": "https://api.openai.com/v1/chat/completions",
        "openai_api_key": "your-openai-key",
        "anthropic_endpoint": "https://api.anthropic.com/v1/messages",
        "anthropic_api_key": "your-anthropic-key",
        "cohere_endpoint": "https://api.cohere.ai/v1/generate",
        "cohere_api_key": "your-cohere-key"
    }
    
    orchestrator = AIProviderOrchestrator(config)
    
    # å»ºç«‹æ¸¬è©¦è«‹æ±‚
    request = TaskRequest(
        task_id="test_001",
        user_id="user@company.com",
        prompt="è«‹åˆ†ææˆ‘å€‘å…¬å¸Q3å­£åº¦çš„éŠ·å”®æ•¸æ“šï¼Œä¸¦æä¾›æ”¹å–„å»ºè­°",
        task_type="business_analysis",
        complexity=TaskComplexity.MEDIUM,
        requirements={
            "temperature": 0.3,
            "max_tokens": 800
        },
        constraints={
            "requires_compliance": True,
            "max_cost_per_request": 0.05,
            "max_response_time": 5.0
        },
        context={
            "user_department": "sales",
            "user_role": "manager"
        }
    )
    
    # åŸ·è¡Œæ™ºèƒ½è·¯ç”±
    result = await orchestrator.route_request(request)
    
    if result["success"]:
        print(f"ä½¿ç”¨æä¾›å•†: {result['provider_used']}")
        print(f"å›æ‡‰æ™‚é–“: {result['response_time']:.2f}ç§’")
        print(f"å›æ‡‰å…§å®¹: {result['response']}")
        print(f"è€ƒæ…®çš„æä¾›å•†: {result['considered_providers']}")
    else:
        print(f"è«‹æ±‚å¤±æ•—: {result['error']}")

if __name__ == "__main__":
    asyncio.run(main())
```

</div>

---

## ğŸ“Š æˆæœ¬æ•ˆç›Šåˆ†æèˆ‡å„ªåŒ–

### ğŸ’° å¤šå¹³å°æˆæœ¬æ¨¡å‹

> ğŸ’¡ **ç™½è©±è§£é‡‹**  
> **ç‚ºä»€éº¼è¦åšæˆæœ¬åˆ†æï¼Ÿ** å°±åƒæ‚¨è¦é–‹ä¸€å®¶é¤å»³ï¼Œä¸èƒ½åªè€ƒæ…®èœå¥½ä¸å¥½åƒï¼Œé‚„è¦è€ƒæ…®é£Ÿææˆæœ¬ã€äººå·¥æˆæœ¬ã€ç§Ÿé‡‘ç­‰ç­‰ï¼Œæ‰èƒ½è¨‚å‡ºåˆç†çš„åƒ¹æ ¼ä¸¦ç¢ºä¿ç²åˆ©ã€‚ä½¿ç”¨AIä¹Ÿæ˜¯ä¸€æ¨£ï¼Œä¸åŒçš„AIå¹³å°å°±åƒä¸åŒç­‰ç´šçš„å»šå¸«ï¼Œèƒ½åŠ›å¥½çš„æ¯”è¼ƒè²´ï¼Œä½†ä¸æ˜¯æ¯é“èœéƒ½éœ€è¦ç±³å…¶æ—ä¸»å»šä¾†åšã€‚æˆæœ¬åˆ†æå¹«åŠ©æ‚¨æ‰¾åˆ°ã€Œæ€§åƒ¹æ¯”æœ€é«˜ã€çš„AIçµ„åˆï¼Œè®“æ‚¨åœ¨é ç®—å…§ç²å¾—æœ€å¥½çš„æ•ˆæœï¼

#### ğŸ” å‹•æ…‹æˆæœ¬å„ªåŒ–ç³»çµ±

<div style="background-color: #FFF8E1; padding: 20px; border-left: 4px solid #FFC107; margin: 20px 0;">

**ä¼æ¥­ç´šæˆæœ¬åˆ†æèˆ‡å„ªåŒ–å¹³å°**
```python
import asyncio
import json
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import pandas as pd
from statistics import mean, median
import logging

@dataclass
class CostModel:
    provider: str
    model: str
    input_cost_per_1k: float
    output_cost_per_1k: float
    setup_cost: float = 0.0
    monthly_minimum: float = 0.0
    free_tier_tokens: int = 0
    additional_fees: Dict[str, float] = None

@dataclass
class UsagePattern:
    pattern_name: str
    daily_requests: int
    avg_input_tokens: int
    avg_output_tokens: int
    peak_hours: List[int]
    seasonal_multiplier: Dict[str, float]
    quality_requirements: Dict[str, float]

class EnterpriseCostOptimizer:
    """ä¼æ¥­ç´šAIæˆæœ¬å„ªåŒ–ç³»çµ±"""
    
    def __init__(self):
        self.cost_models = self._initialize_cost_models()
        self.usage_history = []
        self.optimization_rules = self._load_optimization_rules()
        self.setup_logging()
    
    def setup_logging(self):
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def _initialize_cost_models(self) -> Dict[str, List[CostModel]]:
        """åˆå§‹åŒ–æˆæœ¬æ¨¡å‹"""
        
        return {
            "openai": [
                CostModel(
                    provider="openai",
                    model="gpt-4",
                    input_cost_per_1k=0.03,
                    output_cost_per_1k=0.06,
                    free_tier_tokens=0
                ),
                CostModel(
                    provider="openai",
                    model="gpt-3.5-turbo",
                    input_cost_per_1k=0.001,
                    output_cost_per_1k=0.002,
                    free_tier_tokens=0
                )
            ],
            
            "anthropic": [
                CostModel(
                    provider="anthropic",
                    model="claude-3-opus",
                    input_cost_per_1k=0.015,
                    output_cost_per_1k=0.075,
                    free_tier_tokens=0
                ),
                CostModel(
                    provider="anthropic",
                    model="claude-3-sonnet",
                    input_cost_per_1k=0.003,
                    output_cost_per_1k=0.015,
                    free_tier_tokens=0
                )
            ],
            
            "cohere": [
                CostModel(
                    provider="cohere",
                    model="command",
                    input_cost_per_1k=0.001,
                    output_cost_per_1k=0.002,
                    free_tier_tokens=5000000  # æ¯æœˆ5M free tokens
                )
            ],
            
            "azure": [
                CostModel(
                    provider="azure",
                    model="gpt-4",
                    input_cost_per_1k=0.03,
                    output_cost_per_1k=0.06,
                    setup_cost=100.0,  # éƒ¨ç½²æˆæœ¬
                    monthly_minimum=50.0,  # æœ€ä½æœˆè²»
                    additional_fees={"enterprise_support": 500.0}
                )
            ],
            
            "llama_self_hosted": [
                CostModel(
                    provider="llama",
                    model="llama-2-70b",
                    input_cost_per_1k=0.0,  # ç„¡APIè²»ç”¨
                    output_cost_per_1k=0.0,
                    setup_cost=2000.0,  # ç¡¬é«”å’Œè¨­ç½®æˆæœ¬
                    monthly_minimum=800.0,  # GPUç§Ÿç”¨è²»ç”¨
                    additional_fees={"maintenance": 200.0, "electricity": 150.0}
                )
            ]
        }
    
    def _load_optimization_rules(self) -> Dict:
        """è¼‰å…¥å„ªåŒ–è¦å‰‡"""
        
        return {
            "cost_thresholds": {
                "low_usage": 100.0,      # æœˆè²»ç”¨ < $100
                "medium_usage": 1000.0,  # $100 <= æœˆè²»ç”¨ < $1000
                "high_usage": 5000.0     # $1000 <= æœˆè²»ç”¨ < $5000
            },
            
            "optimization_strategies": {
                "low_usage": {
                    "preferred_providers": ["cohere", "openai:gpt-3.5-turbo"],
                    "avoid_setup_costs": True,
                    "maximize_free_tiers": True
                },
                
                "medium_usage": {
                    "preferred_providers": ["openai", "anthropic:claude-3-sonnet"],
                    "consider_volume_discounts": True,
                    "balance_cost_quality": True
                },
                
                "high_usage": {
                    "preferred_providers": ["azure", "llama_self_hosted"],
                    "setup_costs_acceptable": True,
                    "focus_on_unit_cost": True,
                    "consider_dedicated_instances": True
                }
            },
            
            "task_routing": {
                "simple_tasks": ["cohere", "openai:gpt-3.5-turbo"],
                "complex_analysis": ["anthropic:claude-3-opus", "openai:gpt-4"],
                "high_volume_generation": ["llama_self_hosted", "azure"],
                "enterprise_critical": ["azure", "anthropic"]
            }
        }
    
    async def analyze_usage_patterns(
        self, 
        historical_data: List[Dict]
    ) -> Dict[str, UsagePattern]:
        """åˆ†æä½¿ç”¨æ¨¡å¼"""
        
        patterns = {}
        
        # æŒ‰ä»»å‹™é¡å‹åˆ†çµ„åˆ†æ
        task_groups = {}
        for record in historical_data:
            task_type = record.get("task_type", "general")
            if task_type not in task_groups:
                task_groups[task_type] = []
            task_groups[task_type].append(record)
        
        for task_type, records in task_groups.items():
            # è¨ˆç®—åŸºæœ¬çµ±è¨ˆ
            daily_requests = len(records) / 30  # å‡è¨­30å¤©æ•¸æ“š
            input_tokens = [r.get("input_tokens", 0) for r in records]
            output_tokens = [r.get("output_tokens", 0) for r in records]
            
            # åˆ†ææ™‚é–“æ¨¡å¼
            hours = [datetime.fromisoformat(r["timestamp"]).hour for r in records]
            peak_hours = self._identify_peak_hours(hours)
            
            # å­£ç¯€æ€§åˆ†æï¼ˆç°¡åŒ–ï¼‰
            seasonal_multiplier = {
                "Q1": 1.0, "Q2": 1.2, "Q3": 0.8, "Q4": 1.5
            }
            
            # å“è³ªè¦æ±‚åˆ†æ
            quality_scores = [r.get("quality_score", 0.8) for r in records if r.get("quality_score")]
            
            patterns[task_type] = UsagePattern(
                pattern_name=task_type,
                daily_requests=int(daily_requests),
                avg_input_tokens=int(mean(input_tokens)) if input_tokens else 0,
                avg_output_tokens=int(mean(output_tokens)) if output_tokens else 0,
                peak_hours=peak_hours,
                seasonal_multiplier=seasonal_multiplier,
                quality_requirements={
                    "min_quality": min(quality_scores) if quality_scores else 0.7,
                    "avg_quality": mean(quality_scores) if quality_scores else 0.8,
                    "target_quality": 0.9
                }
            )
        
        return patterns
    
    def _identify_peak_hours(self, hours: List[int]) -> List[int]:
        """è­˜åˆ¥å°–å³°æ™‚æ®µ"""
        
        hour_counts = {}
        for hour in hours:
            hour_counts[hour] = hour_counts.get(hour, 0) + 1
        
        # æ‰¾å‡ºä½¿ç”¨é‡æœ€é«˜çš„æ™‚æ®µ
        avg_count = mean(hour_counts.values())
        peak_hours = [hour for hour, count in hour_counts.items() 
                     if count > avg_count * 1.5]
        
        return sorted(peak_hours)
    
    async def calculate_monthly_costs(
        self, 
        usage_patterns: Dict[str, UsagePattern]
    ) -> Dict:
        """è¨ˆç®—æœˆåº¦æˆæœ¬"""
        
        cost_analysis = {
            "by_provider": {},
            "by_task_type": {},
            "total_comparison": {},
            "optimization_opportunities": []
        }
        
        # è¨ˆç®—æ¯å€‹æä¾›å•†çš„æˆæœ¬
        for provider, models in self.cost_models.items():
            cost_analysis["by_provider"][provider] = {}
            
            for model in models:
                monthly_cost = await self._calculate_provider_monthly_cost(
                    model, usage_patterns
                )
                cost_analysis["by_provider"][provider][model.model] = monthly_cost
        
        # è¨ˆç®—æ¯å€‹ä»»å‹™é¡å‹çš„æœ€ä½³æ–¹æ¡ˆ
        for task_type, pattern in usage_patterns.items():
            best_options = await self._find_best_cost_options(
                pattern, top_n=3
            )
            cost_analysis["by_task_type"][task_type] = best_options
        
        # ç¸½é«”æˆæœ¬æ¯”è¼ƒ
        cost_analysis["total_comparison"] = await self._compare_total_costs(
            usage_patterns
        )
        
        # å„ªåŒ–å»ºè­°
        cost_analysis["optimization_opportunities"] = await self._identify_optimization_opportunities(
            usage_patterns, cost_analysis
        )
        
        return cost_analysis
    
    async def _calculate_provider_monthly_cost(
        self, 
        model: CostModel, 
        usage_patterns: Dict[str, UsagePattern]
    ) -> Dict:
        """è¨ˆç®—ç‰¹å®šæä¾›å•†æ¨¡å‹çš„æœˆåº¦æˆæœ¬"""
        
        total_input_tokens = 0
        total_output_tokens = 0
        total_requests = 0
        
        # å½™ç¸½æ‰€æœ‰ä»»å‹™é¡å‹çš„ä½¿ç”¨é‡
        for pattern in usage_patterns.values():
            monthly_requests = pattern.daily_requests * 30
            total_requests += monthly_requests
            total_input_tokens += monthly_requests * pattern.avg_input_tokens
            total_output_tokens += monthly_requests * pattern.avg_output_tokens
        
        # è¨ˆç®—tokenæˆæœ¬
        input_cost = 0
        output_cost = 0
        
        # è€ƒæ…®å…è²»é¡åº¦
        if model.free_tier_tokens > 0:
            free_input_tokens = min(total_input_tokens, model.free_tier_tokens // 2)
            free_output_tokens = min(total_output_tokens, model.free_tier_tokens // 2)
            
            paid_input_tokens = max(0, total_input_tokens - free_input_tokens)
            paid_output_tokens = max(0, total_output_tokens - free_output_tokens)
        else:
            paid_input_tokens = total_input_tokens
            paid_output_tokens = total_output_tokens
        
        input_cost = (paid_input_tokens / 1000) * model.input_cost_per_1k
        output_cost = (paid_output_tokens / 1000) * model.output_cost_per_1k
        
        # é¡å¤–è²»ç”¨
        additional_cost = sum(model.additional_fees.values()) if model.additional_fees else 0
        
        # ç¸½æˆæœ¬
        total_monthly_cost = max(
            input_cost + output_cost + additional_cost,
            model.monthly_minimum
        ) + (model.setup_cost / 12)  # æ”¤éŠ·è¨­ç½®æˆæœ¬
        
        return {
            "total_cost": round(total_monthly_cost, 2),
            "input_cost": round(input_cost, 2),
            "output_cost": round(output_cost, 2),
            "additional_cost": round(additional_cost, 2),
            "setup_cost_monthly": round(model.setup_cost / 12, 2),
            "total_tokens": total_input_tokens + total_output_tokens,
            "cost_per_1k_tokens": round(total_monthly_cost / ((total_input_tokens + total_output_tokens) / 1000), 4) if total_input_tokens + total_output_tokens > 0 else 0,
            "cost_per_request": round(total_monthly_cost / total_requests, 4) if total_requests > 0 else 0
        }
    
    async def _find_best_cost_options(
        self, 
        pattern: UsagePattern,
        top_n: int = 3
    ) -> List[Dict]:
        """æ‰¾å‡ºæœ€ä½³æˆæœ¬é¸é …"""
        
        options = []
        
        for provider, models in self.cost_models.items():
            for model in models:
                # ç°¡åŒ–çš„å–®ä¸€ä»»å‹™æˆæœ¬è¨ˆç®—
                monthly_requests = pattern.daily_requests * 30
                input_tokens = monthly_requests * pattern.avg_input_tokens
                output_tokens = monthly_requests * pattern.avg_output_tokens
                
                # æˆæœ¬è¨ˆç®—
                input_cost = (input_tokens / 1000) * model.input_cost_per_1k
                output_cost = (output_tokens / 1000) * model.output_cost_per_1k
                additional_cost = sum(model.additional_fees.values()) if model.additional_fees else 0
                
                total_cost = max(
                    input_cost + output_cost + additional_cost,
                    model.monthly_minimum
                ) + (model.setup_cost / 12)
                
                # å“è³ªè©•ä¼°ï¼ˆç°¡åŒ–ï¼‰
                quality_score = self._estimate_quality_score(model, pattern)
                
                # æ€§åƒ¹æ¯”è¨ˆç®—
                value_score = quality_score / total_cost if total_cost > 0 else 0
                
                options.append({
                    "provider": model.provider,
                    "model": model.model,
                    "monthly_cost": round(total_cost, 2),
                    "cost_per_request": round(total_cost / monthly_requests, 4) if monthly_requests > 0 else 0,
                    "estimated_quality": quality_score,
                    "value_score": round(value_score, 4),
                    "setup_required": model.setup_cost > 0,
                    "free_tier_benefit": model.free_tier_tokens > 0
                })
        
        # æŒ‰æ€§åƒ¹æ¯”æ’åº
        options.sort(key=lambda x: x["value_score"], reverse=True)
        
        return options[:top_n]
    
    def _estimate_quality_score(self, model: CostModel, pattern: UsagePattern) -> float:
        """ä¼°è¨ˆå“è³ªåˆ†æ•¸ï¼ˆç°¡åŒ–æ¨¡å‹ï¼‰"""
        
        # åŸºæ–¼æ¨¡å‹è¤‡é›œåº¦çš„å“è³ªä¼°è¨ˆ
        quality_mapping = {
            "gpt-4": 0.95,
            "claude-3-opus": 0.94,
            "claude-3-sonnet": 0.88,
            "gpt-3.5-turbo": 0.82,
            "command": 0.80,
            "llama-2-70b": 0.78
        }
        
        base_quality = quality_mapping.get(model.model, 0.75)
        
        # æ ¹æ“šä»»å‹™éœ€æ±‚èª¿æ•´
        required_quality = pattern.quality_requirements.get("target_quality", 0.8)
        if base_quality < required_quality:
            return base_quality * 0.8  # é™ä½ä¸ç¬¦åˆéœ€æ±‚çš„åˆ†æ•¸
        
        return base_quality
    
    async def _compare_total_costs(
        self, 
        usage_patterns: Dict[str, UsagePattern]
    ) -> Dict:
        """æ¯”è¼ƒç¸½é«”æˆæœ¬"""
        
        strategies = {
            "single_provider_premium": {
                "description": "ä½¿ç”¨å–®ä¸€é«˜å“è³ªæä¾›å•†",
                "assignments": {"*": "openai:gpt-4"}
            },
            
            "single_provider_balanced": {
                "description": "ä½¿ç”¨å–®ä¸€å¹³è¡¡å‹æä¾›å•†", 
                "assignments": {"*": "anthropic:claude-3-sonnet"}
            },
            
            "cost_optimized_mix": {
                "description": "æˆæœ¬å„ªåŒ–æ··åˆç­–ç•¥",
                "assignments": {
                    "simple_tasks": "cohere:command",
                    "complex_analysis": "anthropic:claude-3-opus",
                    "code_generation": "openai:gpt-4",
                    "high_volume": "llama_self_hosted:llama-2-70b"
                }
            },
            
            "quality_optimized_mix": {
                "description": "å“è³ªå„ªåŒ–æ··åˆç­–ç•¥",
                "assignments": {
                    "simple_tasks": "openai:gpt-3.5-turbo",
                    "complex_analysis": "anthropic:claude-3-opus", 
                    "code_generation": "openai:gpt-4",
                    "high_volume": "azure:gpt-4"
                }
            }
        }
        
        comparison_results = {}
        
        for strategy_name, strategy in strategies.items():
            total_cost = 0
            weighted_quality = 0
            total_requests = 0
            
            for task_type, pattern in usage_patterns.items():
                # ç¢ºå®šä½¿ç”¨çš„æ¨¡å‹
                if task_type in strategy["assignments"]:
                    provider_model = strategy["assignments"][task_type]
                else:
                    provider_model = strategy["assignments"]["*"]
                
                provider, model_name = provider_model.split(":")
                model = next(
                    (m for m in self.cost_models[provider] if m.model == model_name),
                    None
                )
                
                if model:
                    # è¨ˆç®—æˆæœ¬
                    cost_info = await self._calculate_provider_monthly_cost(
                        model, {task_type: pattern}
                    )
                    
                    task_cost = cost_info["total_cost"]
                    task_requests = pattern.daily_requests * 30
                    task_quality = self._estimate_quality_score(model, pattern)
                    
                    total_cost += task_cost
                    weighted_quality += task_quality * task_requests
                    total_requests += task_requests
            
            avg_quality = weighted_quality / total_requests if total_requests > 0 else 0
            
            comparison_results[strategy_name] = {
                "description": strategy["description"],
                "total_monthly_cost": round(total_cost, 2),
                "average_quality": round(avg_quality, 3),
                "cost_per_request": round(total_cost / total_requests, 4) if total_requests > 0 else 0,
                "value_score": round(avg_quality / total_cost * 1000, 2) if total_cost > 0 else 0
            }
        
        return comparison_results
    
    async def _identify_optimization_opportunities(
        self, 
        usage_patterns: Dict[str, UsagePattern],
        cost_analysis: Dict
    ) -> List[Dict]:
        """è­˜åˆ¥å„ªåŒ–æ©Ÿæœƒ"""
        
        opportunities = []
        
        # åˆ†æç•¶å‰ç¸½æˆæœ¬
        total_current_cost = sum(
            min(options[0]["monthly_cost"] for options in task_costs.values())
            for task_costs in cost_analysis["by_task_type"].values()
        )
        
        # æ©Ÿæœƒ1ï¼šå…è²»é¡åº¦åˆ©ç”¨
        for provider, models in self.cost_models.items():
            for model in models:
                if model.free_tier_tokens > 0:
                    opportunities.append({
                        "type": "free_tier_utilization",
                        "description": f"åˆ©ç”¨{provider}çš„å…è²»é¡åº¦({model.free_tier_tokens:,} tokens)",
                        "potential_savings": "æœ€é«˜å¯ç¯€çœ50%åˆæœŸæˆæœ¬",
                        "requirements": ["è¨»å†Šå¸³æˆ¶", "ç›£æ§ç”¨é‡é™åˆ¶"],
                        "risk_level": "ä½"
                    })
        
        # æ©Ÿæœƒ2ï¼šä»»å‹™åˆ†å±¤ç­–ç•¥
        if len(usage_patterns) > 1:
            opportunities.append({
                "type": "task_stratification",
                "description": "æ ¹æ“šä»»å‹™è¤‡é›œåº¦é¸æ“‡ä¸åŒç­‰ç´šçš„æ¨¡å‹",
                "potential_savings": f"é ä¼°ç¯€çœ20-40%ï¼ˆç´„${total_current_cost * 0.3:.0f}/æœˆï¼‰",
                "requirements": ["å¯¦æ–½æ™ºèƒ½è·¯ç”±", "ä»»å‹™åˆ†é¡ç³»çµ±"],
                "risk_level": "ä¸­"
            })
        
        # æ©Ÿæœƒ3ï¼šè‡ªå»ºéƒ¨ç½²
        high_volume_tasks = [
            name for name, pattern in usage_patterns.items()
            if pattern.daily_requests > 1000
        ]
        
        if high_volume_tasks:
            opportunities.append({
                "type": "self_hosted_deployment",
                "description": "é«˜ç”¨é‡ä»»å‹™è€ƒæ…®è‡ªå»ºLlamaéƒ¨ç½²",
                "potential_savings": f"é•·æœŸå¯ç¯€çœ60%ä»¥ä¸Šï¼ˆæœˆç”¨é‡>10è¬æ¬¡è«‹æ±‚ï¼‰",
                "requirements": ["åˆæœŸæŠ•è³‡$2000", "æŠ€è¡“ç¶­è­·èƒ½åŠ›", "GPUè³‡æº"],
                "risk_level": "é«˜",
                "break_even_point": "6-12å€‹æœˆ"
            })
        
        # æ©Ÿæœƒ4ï¼šæ‰¹é‡æŠ˜æ‰£
        if total_current_cost > 1000:
            opportunities.append({
                "type": "volume_discount",
                "description": "èˆ‡æä¾›å•†è«‡åˆ¤å¤§ç”¨é‡æŠ˜æ‰£",
                "potential_savings": "5-15%çš„åƒ¹æ ¼å„ªæƒ ",
                "requirements": ["ä¼æ¥­åˆç´„è«‡åˆ¤", "æ‰¿è«¾æœ€ä½ç”¨é‡"],
                "risk_level": "ä½"
            })
        
        return opportunities
    
    def generate_cost_report(
        self, 
        cost_analysis: Dict,
        usage_patterns: Dict[str, UsagePattern]
    ) -> str:
        """ç”Ÿæˆæˆæœ¬åˆ†æå ±å‘Š"""
        
        report = f"""
# AIæˆæœ¬åˆ†æå ±å‘Š
ç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## åŸ·è¡Œæ‘˜è¦
- åˆ†æäº† {len(usage_patterns)} ç¨®ä½¿ç”¨æ¨¡å¼
- æ¯”è¼ƒäº† {len(self.cost_models)} å€‹AIæä¾›å•†
- è­˜åˆ¥äº† {len(cost_analysis['optimization_opportunities'])} å€‹å„ªåŒ–æ©Ÿæœƒ

## ä½¿ç”¨æ¨¡å¼æ¦‚è¦½
"""
        
        for task_type, pattern in usage_patterns.items():
            monthly_requests = pattern.daily_requests * 30
            monthly_tokens = monthly_requests * (pattern.avg_input_tokens + pattern.avg_output_tokens)
            
            report += f"""
### {task_type}
- æœˆåº¦è«‹æ±‚é‡ï¼š{monthly_requests:,} æ¬¡
- æœˆåº¦Tokenç”¨é‡ï¼š{monthly_tokens:,} tokens
- å°–å³°æ™‚æ®µï¼š{', '.join(map(str, pattern.peak_hours))}:00
- å“è³ªè¦æ±‚ï¼š{pattern.quality_requirements['target_quality']:.1%}
"""
        
        report += "\n## æˆæœ¬æ¯”è¼ƒ\n"
        
        for strategy, results in cost_analysis["total_comparison"].items():
            report += f"""
### {results['description']}
- æœˆåº¦ç¸½æˆæœ¬ï¼š${results['total_monthly_cost']:,.2f}
- å¹³å‡å“è³ªåˆ†æ•¸ï¼š{results['average_quality']:.3f}
- æ¯æ¬¡è«‹æ±‚æˆæœ¬ï¼š${results['cost_per_request']:.4f}
- æ€§åƒ¹æ¯”è©•åˆ†ï¼š{results['value_score']:.2f}
"""
        
        report += "\n## å„ªåŒ–å»ºè­°\n"
        
        for i, opportunity in enumerate(cost_analysis["optimization_opportunities"], 1):
            report += f"""
### {i}. {opportunity['description']}
- é¡å‹ï¼š{opportunity['type']}
- æ½›åœ¨ç¯€çœï¼š{opportunity['potential_savings']}
- å¯¦æ–½è¦æ±‚ï¼š{', '.join(opportunity['requirements'])}
- é¢¨éšªç­‰ç´šï¼š{opportunity['risk_level']}
"""
            
            if 'break_even_point' in opportunity:
                report += f"- å›æœ¬é€±æœŸï¼š{opportunity['break_even_point']}\n"
        
        return report

# ä½¿ç”¨ç¯„ä¾‹
async def main():
    optimizer = EnterpriseCostOptimizer()
    
    # æ¨¡æ“¬æ­·å²ä½¿ç”¨æ•¸æ“š
    historical_data = [
        {
            "timestamp": "2024-01-15T10:30:00",
            "task_type": "customer_service",
            "input_tokens": 150,
            "output_tokens": 200,
            "quality_score": 0.85
        },
        {
            "timestamp": "2024-01-15T14:20:00", 
            "task_type": "business_analysis",
            "input_tokens": 800,
            "output_tokens": 600,
            "quality_score": 0.92
        }
        # æ›´å¤šæ•¸æ“š...
    ] * 100  # æ¨¡æ“¬æ›´å¤šæ•¸æ“š
    
    # åˆ†æä½¿ç”¨æ¨¡å¼
    usage_patterns = await optimizer.analyze_usage_patterns(historical_data)
    
    # è¨ˆç®—æˆæœ¬
    cost_analysis = await optimizer.calculate_monthly_costs(usage_patterns)
    
    # ç”Ÿæˆå ±å‘Š
    report = optimizer.generate_cost_report(cost_analysis, usage_patterns)
    print(report)
    
    # ä¿å­˜åˆ†æçµæœ
    with open("cost_analysis_report.json", "w", encoding="utf-8") as f:
        json.dump({
            "usage_patterns": {k: asdict(v) for k, v in usage_patterns.items()},
            "cost_analysis": cost_analysis,
            "generated_at": datetime.now().isoformat()
        }, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    asyncio.run(main())
```

</div>

---

## ğŸ’¡ é—œéµè¦é»ç¸½çµ

<div style="background-color: #F0F4C3; padding: 20px; border-left: 4px solid #CDDC39; margin: 20px 0;">

### ğŸ¯ å¹³å°é¸æ“‡ç­–ç•¥
1. **é–‹æºéœ€æ±‚**ï¼šMeta Llamaæä¾›å®Œå…¨å¯æ§çš„é–‹æºè§£æ±ºæ–¹æ¡ˆ
2. **ä¼æ¥­åˆè¦**ï¼šAzure AIå…·å‚™æœ€å®Œæ•´çš„ä¼æ¥­ç´šå®‰å…¨å’Œåˆè¦åŠŸèƒ½
3. **æˆæœ¬æ•ˆç›Š**ï¼šCohereå’ŒHugging Faceåœ¨ç‰¹å®šå ´æ™¯ä¸‹æ€§åƒ¹æ¯”æœ€é«˜
4. **æŠ€è¡“å‰µæ–°**ï¼šå„å¹³å°éƒ½æœ‰ç¨ç‰¹çš„æŠ€è¡“å„ªå‹¢ï¼Œéœ€è¦æ ¹æ“šå…·é«”éœ€æ±‚é¸æ“‡

### ğŸ› ï¸ æ•´åˆæœ€ä½³å¯¦è¸
1. **æ™ºèƒ½ç·¨æ’**ï¼šå»ºç«‹è·¨å¹³å°çš„æ™ºèƒ½è·¯ç”±å’Œæ•…éšœè½‰ç§»æ©Ÿåˆ¶
2. **æˆæœ¬å„ªåŒ–**ï¼šå¯¦æ–½å‹•æ…‹æˆæœ¬åˆ†æå’Œå¤šå±¤æ¬¡å„ªåŒ–ç­–ç•¥
3. **å“è³ªä¿è­‰**ï¼šå»ºç«‹çµ±ä¸€çš„å“è³ªè©•ä¼°å’Œç›£æ§æ¨™æº–
4. **å®‰å…¨åˆè¦**ï¼šç¢ºä¿æ‰€æœ‰å¹³å°éƒ½ç¬¦åˆä¼æ¥­ç´šå®‰å…¨è¦æ±‚

### ğŸ“ˆ æœªä¾†ç™¼å±•è¶¨å‹¢
1. **æ¨™æº–åŒ–é€²ç¨‹**ï¼šAPIæ¥å£å’ŒåŠŸèƒ½é€æ­¥æ¨™æº–åŒ–
2. **å°ˆæ¥­åˆ†å·¥**ï¼šä¸åŒæä¾›å•†åœ¨ç‰¹å®šé ˜åŸŸçš„å°ˆæ¥­åŒ–ç¨‹åº¦æå‡
3. **æˆæœ¬ç«¶çˆ­**ï¼šæ¿€çƒˆçš„åƒ¹æ ¼ç«¶çˆ­å¸¶ä¾†æ›´å¥½çš„æˆæœ¬æ•ˆç›Š
4. **ç”Ÿæ…‹æ•´åˆ**ï¼šå¤šæ¨¡æ…‹å’Œè·¨å¹³å°æ•´åˆæˆç‚ºä¸»æµè¶¨å‹¢

### ğŸ”® ä¼æ¥­æ±ºç­–æŒ‡å—
1. **è©•ä¼°ç¾ç‹€**ï¼šåˆ†æç•¶å‰AIä½¿ç”¨æƒ…æ³å’Œæ¥­å‹™éœ€æ±‚
2. **åˆ¶å®šç­–ç•¥**ï¼šæ ¹æ“šæˆæœ¬ã€å“è³ªã€åˆè¦è¦æ±‚åˆ¶å®šå¤šå¹³å°ç­–ç•¥
3. **åˆ†éšæ®µå¯¦æ–½**ï¼šå¾ä½é¢¨éšªå ´æ™¯é–‹å§‹ï¼Œé€æ­¥æ“´å±•åˆ°æ ¸å¿ƒæ¥­å‹™
4. **æŒçºŒå„ªåŒ–**ï¼šå»ºç«‹ç›£æ§å’Œå„ªåŒ–æ©Ÿåˆ¶ï¼ŒæŒçºŒæ”¹é€²æ•ˆæœå’Œæˆæœ¬æ•ˆç›Š

</div>

---

<p align="center">
<strong>ğŸš€ æ­å–œæ‚¨å®Œæˆå…¶ä»–LLMæä¾›å•†æŒ‡å—å­¸ç¿’ï¼</strong><br>
<em>æ¥ä¸‹ä¾†ï¼Œè®“æˆ‘å€‘æ·±å…¥æ¢è¨LLMç‰¹æ€§èˆ‡æ•™å­¸æ‡‰ç”¨</em>
</p>

<p align="center">
<a href="./07-LLMç‰¹æ€§èˆ‡æ•™å­¸æŒ‡å—.md">
<img src="https://img.shields.io/badge/ä¸‹ä¸€ç« -LLMç‰¹æ€§èˆ‡æ•™å­¸-blue?style=for-the-badge" alt="ä¸‹ä¸€ç« ">
</a>
<a href="./05-Geminiæç¤ºå·¥ç¨‹æŒ‡å—.md">
<img src="https://img.shields.io/badge/å›é¡§-GeminiæŒ‡å—-green?style=for-the-badge" alt="GeminiæŒ‡å—">
</a>
<a href="./README.md">
<img src="https://img.shields.io/badge/è¿”å›-ä¸»é -orange?style=for-the-badge" alt="è¿”å›ä¸»é ">
</a>
</p>